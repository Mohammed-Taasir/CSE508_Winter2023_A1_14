{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzQGrvoE1B52k37onmKnJN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammed-Taasir/IR_2023-GROUP-14/blob/assignment-1/IR_Assignment_1_QUES_1_and_QUES_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWAYXQ0oSqZ2",
        "outputId": "3d9d432d-daa7-4d33-dd68-347cf91b8679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/IR Assignment-1/CSE508_Winter2023_Dataset.zip' '/content/'\n",
        "!unzip 'CSE508_Winter2023_Dataset.zip' &> /dev/null\n",
        "!rm 'CSE508_Winter2023_Dataset.zip'"
      ],
      "metadata": {
        "id": "ASJxIYUHrJVn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import re\n",
        "import pickle"
      ],
      "metadata": {
        "id": "PD1AsTJzSxYQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sortedcontainers import SortedDict, SortedList, SortedSet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import PlaintextCorpusReader \n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "On3RRYHU765j"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * Defining Helper Functions"
      ],
      "metadata": {
        "id": "2KjMCCfaxY1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read from files"
      ],
      "metadata": {
        "id": "AAWglbrwxfUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getListOfFiles(directory):\n",
        "    '''\n",
        "    Parameters:\n",
        "        directory: type(string)\n",
        "        \n",
        "    returns: list of all files in directory with the full path of file\n",
        "    '''\n",
        "    \n",
        "    list_of_files = []\n",
        "    \n",
        "    for file_path in os.listdir(directory):\n",
        "        full_path = os.path.join(directory, file_path)\n",
        "        if os.path.isfile(full_path):\n",
        "            list_of_files.append(full_path)\n",
        "    \n",
        "    return list_of_files"
      ],
      "metadata": {
        "id": "9YvT7YP0xkSg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Functions"
      ],
      "metadata": {
        "id": "cn9gfxflxrPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(data):\n",
        "    '''\n",
        "    Parameters:\n",
        "        data: type(string)\n",
        "    \n",
        "    returns: lowercase of data\n",
        "    '''\n",
        "    \n",
        "    return data.lower()"
      ],
      "metadata": {
        "id": "5nC5p6-DxkQk"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_word_tokenize(corpus):\n",
        "    '''\n",
        "    Parameters:\n",
        "        corpus: type(string)\n",
        "    \n",
        "    returns word-level tokenization of corpus\n",
        "    '''\n",
        "    \n",
        "    return word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "DRiBql_LxkL6"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_from_tokens(tokens, stopwords_set):\n",
        "    '''\n",
        "    Parameters:\n",
        "        tokens: type(list)\n",
        "        stopwords_set: type(set)\n",
        "    \n",
        "    returns: tokens without stopwords\n",
        "    '''\n",
        "    tokens_sans_stopwords = [x for x in tokens if x not in stopwords_set]\n",
        "    \n",
        "    return tokens_sans_stopwords"
      ],
      "metadata": {
        "id": "WNEgOPT-xzTs"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation_from_tokens(tokens):\n",
        "    '''\n",
        "    Parameters:\n",
        "        tokens: type(list)\n",
        "    \n",
        "    returns: tokens without punctuation\n",
        "    '''\n",
        "    tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in tokens]\n",
        "    \n",
        "    return tokens_sans_punctuation"
      ],
      "metadata": {
        "id": "2slNeuK_xzPk"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_blank_space_tokens(tokens):\n",
        "    '''\n",
        "    Parameters:\n",
        "        tokens: type(list)\n",
        "    \n",
        "    returns: tokens without blank tokens\n",
        "    '''\n",
        "    tokens_sans_blank_space = [x for x in tokens if x!='']\n",
        "    \n",
        "    return tokens_sans_blank_space"
      ],
      "metadata": {
        "id": "eww-0FRzxzN3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, stopwords_set, preprocess_type):\n",
        "    # Convert the text to lower case\n",
        "    lowercase_corpus = lowercase(corpus)\n",
        "    #print(len(lowercase_corpus))\n",
        "    \n",
        "    # Perform word tokenization (word_tokenize also takes care of whitespace)\n",
        "    word_tokens = perform_word_tokenize(lowercase_corpus)\n",
        "    #print(len(word_tokens))\n",
        "    \n",
        "    # Remove stopwords from tokens\n",
        "    word_tokens_sans_stopwords = remove_stopwords_from_tokens(word_tokens, stopwords_set)\n",
        "    #print(len(word_tokens_sans_stopwords))\n",
        "    \n",
        "    # Remove punctuation marks from tokens\n",
        "    word_tokens_sans_punctuation = remove_punctuation_from_tokens(word_tokens_sans_stopwords)\n",
        "    #print(len(word_tokens_sans_punctuation))\n",
        "    \n",
        "    # Remove blank space tokens\n",
        "    word_tokens_sans_blank_tokens = remove_blank_space_tokens(word_tokens_sans_punctuation)\n",
        "    #print(len(word_tokens_sans_blank_tokens))\n",
        "  \n",
        "    \n",
        "    if preprocess_type=='query':\n",
        "        return word_tokens_sans_blank_tokens    # if its a query return tokens in the same order\n",
        "    return sorted(list(dict.fromkeys(word_tokens_sans_blank_tokens)))   # if its a whole document then return tokens in sorted manner"
      ],
      "metadata": {
        "id": "i1-N23x8xzJs"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file_dictionary(list_of_files):\n",
        "    '''\n",
        "    Paramteres:\n",
        "        list_of_files: type(string)\n",
        "    \n",
        "    returns: file_dictionary with integer key and path_of_file as value\n",
        "    '''\n",
        "    file_dictionary = {}\n",
        "    for i in range(len(list_of_files)):\n",
        "        file_dictionary[i] = list_of_files[i]\n",
        "    \n",
        "    return file_dictionary"
      ],
      "metadata": {
        "id": "F54608saxzIC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9hec8Llgxj5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Vo41vetTxj35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Data Preprocessing"
      ],
      "metadata": {
        "id": "B92O1UIg32r7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (i) Relevant Text Extraction"
      ],
      "metadata": {
        "id": "tMm3u5Gi36oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_root='/content/CSE508_Winter2023_Dataset'\n",
        "corpus=PlaintextCorpusReader(corpus_root,'.*')"
      ],
      "metadata": {
        "id": "CjjuqTawiSUE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filenames=corpus.fileids()\n",
        "print(filenames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWPmNl4-ifMA",
        "outputId": "e02aefac-4ab9-43d3-fd3f-bd930f1f7c81"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cranfield0001', 'cranfield0002', 'cranfield0003', 'cranfield0004', 'cranfield0005', 'cranfield0006', 'cranfield0007', 'cranfield0008', 'cranfield0009', 'cranfield0010', 'cranfield0011', 'cranfield0012', 'cranfield0013', 'cranfield0014', 'cranfield0015', 'cranfield0016', 'cranfield0017', 'cranfield0018', 'cranfield0019', 'cranfield0020', 'cranfield0021', 'cranfield0022', 'cranfield0023', 'cranfield0024', 'cranfield0025', 'cranfield0026', 'cranfield0027', 'cranfield0028', 'cranfield0029', 'cranfield0030', 'cranfield0031', 'cranfield0032', 'cranfield0033', 'cranfield0034', 'cranfield0035', 'cranfield0036', 'cranfield0037', 'cranfield0038', 'cranfield0039', 'cranfield0040', 'cranfield0041', 'cranfield0042', 'cranfield0043', 'cranfield0044', 'cranfield0045', 'cranfield0046', 'cranfield0047', 'cranfield0048', 'cranfield0049', 'cranfield0050', 'cranfield0051', 'cranfield0052', 'cranfield0053', 'cranfield0054', 'cranfield0055', 'cranfield0056', 'cranfield0057', 'cranfield0058', 'cranfield0059', 'cranfield0060', 'cranfield0061', 'cranfield0062', 'cranfield0063', 'cranfield0064', 'cranfield0065', 'cranfield0066', 'cranfield0067', 'cranfield0068', 'cranfield0069', 'cranfield0070', 'cranfield0071', 'cranfield0072', 'cranfield0073', 'cranfield0074', 'cranfield0075', 'cranfield0076', 'cranfield0077', 'cranfield0078', 'cranfield0079', 'cranfield0080', 'cranfield0081', 'cranfield0082', 'cranfield0083', 'cranfield0084', 'cranfield0085', 'cranfield0086', 'cranfield0087', 'cranfield0088', 'cranfield0089', 'cranfield0090', 'cranfield0091', 'cranfield0092', 'cranfield0093', 'cranfield0094', 'cranfield0095', 'cranfield0096', 'cranfield0097', 'cranfield0098', 'cranfield0099', 'cranfield0100', 'cranfield0101', 'cranfield0102', 'cranfield0103', 'cranfield0104', 'cranfield0105', 'cranfield0106', 'cranfield0107', 'cranfield0108', 'cranfield0109', 'cranfield0110', 'cranfield0111', 'cranfield0112', 'cranfield0113', 'cranfield0114', 'cranfield0115', 'cranfield0116', 'cranfield0117', 'cranfield0118', 'cranfield0119', 'cranfield0120', 'cranfield0121', 'cranfield0122', 'cranfield0123', 'cranfield0124', 'cranfield0125', 'cranfield0126', 'cranfield0127', 'cranfield0128', 'cranfield0129', 'cranfield0130', 'cranfield0131', 'cranfield0132', 'cranfield0133', 'cranfield0134', 'cranfield0135', 'cranfield0136', 'cranfield0137', 'cranfield0138', 'cranfield0139', 'cranfield0140', 'cranfield0141', 'cranfield0142', 'cranfield0143', 'cranfield0144', 'cranfield0145', 'cranfield0146', 'cranfield0147', 'cranfield0148', 'cranfield0149', 'cranfield0150', 'cranfield0151', 'cranfield0152', 'cranfield0153', 'cranfield0154', 'cranfield0155', 'cranfield0156', 'cranfield0157', 'cranfield0158', 'cranfield0159', 'cranfield0160', 'cranfield0161', 'cranfield0162', 'cranfield0163', 'cranfield0164', 'cranfield0165', 'cranfield0166', 'cranfield0167', 'cranfield0168', 'cranfield0169', 'cranfield0170', 'cranfield0171', 'cranfield0172', 'cranfield0173', 'cranfield0174', 'cranfield0175', 'cranfield0176', 'cranfield0177', 'cranfield0178', 'cranfield0179', 'cranfield0180', 'cranfield0181', 'cranfield0182', 'cranfield0183', 'cranfield0184', 'cranfield0185', 'cranfield0186', 'cranfield0187', 'cranfield0188', 'cranfield0189', 'cranfield0190', 'cranfield0191', 'cranfield0192', 'cranfield0193', 'cranfield0194', 'cranfield0195', 'cranfield0196', 'cranfield0197', 'cranfield0198', 'cranfield0199', 'cranfield0200', 'cranfield0201', 'cranfield0202', 'cranfield0203', 'cranfield0204', 'cranfield0205', 'cranfield0206', 'cranfield0207', 'cranfield0208', 'cranfield0209', 'cranfield0210', 'cranfield0211', 'cranfield0212', 'cranfield0213', 'cranfield0214', 'cranfield0215', 'cranfield0216', 'cranfield0217', 'cranfield0218', 'cranfield0219', 'cranfield0220', 'cranfield0221', 'cranfield0222', 'cranfield0223', 'cranfield0224', 'cranfield0225', 'cranfield0226', 'cranfield0227', 'cranfield0228', 'cranfield0229', 'cranfield0230', 'cranfield0231', 'cranfield0232', 'cranfield0233', 'cranfield0234', 'cranfield0235', 'cranfield0236', 'cranfield0237', 'cranfield0238', 'cranfield0239', 'cranfield0240', 'cranfield0241', 'cranfield0242', 'cranfield0243', 'cranfield0244', 'cranfield0245', 'cranfield0246', 'cranfield0247', 'cranfield0248', 'cranfield0249', 'cranfield0250', 'cranfield0251', 'cranfield0252', 'cranfield0253', 'cranfield0254', 'cranfield0255', 'cranfield0256', 'cranfield0257', 'cranfield0258', 'cranfield0259', 'cranfield0260', 'cranfield0261', 'cranfield0262', 'cranfield0263', 'cranfield0264', 'cranfield0265', 'cranfield0266', 'cranfield0267', 'cranfield0268', 'cranfield0269', 'cranfield0270', 'cranfield0271', 'cranfield0272', 'cranfield0273', 'cranfield0274', 'cranfield0275', 'cranfield0276', 'cranfield0277', 'cranfield0278', 'cranfield0279', 'cranfield0280', 'cranfield0281', 'cranfield0282', 'cranfield0283', 'cranfield0284', 'cranfield0285', 'cranfield0286', 'cranfield0287', 'cranfield0288', 'cranfield0289', 'cranfield0290', 'cranfield0291', 'cranfield0292', 'cranfield0293', 'cranfield0294', 'cranfield0295', 'cranfield0296', 'cranfield0297', 'cranfield0298', 'cranfield0299', 'cranfield0300', 'cranfield0301', 'cranfield0302', 'cranfield0303', 'cranfield0304', 'cranfield0305', 'cranfield0306', 'cranfield0307', 'cranfield0308', 'cranfield0309', 'cranfield0310', 'cranfield0311', 'cranfield0312', 'cranfield0313', 'cranfield0314', 'cranfield0315', 'cranfield0316', 'cranfield0317', 'cranfield0318', 'cranfield0319', 'cranfield0320', 'cranfield0321', 'cranfield0322', 'cranfield0323', 'cranfield0324', 'cranfield0325', 'cranfield0326', 'cranfield0327', 'cranfield0328', 'cranfield0329', 'cranfield0330', 'cranfield0331', 'cranfield0332', 'cranfield0333', 'cranfield0334', 'cranfield0335', 'cranfield0336', 'cranfield0337', 'cranfield0338', 'cranfield0339', 'cranfield0340', 'cranfield0341', 'cranfield0342', 'cranfield0343', 'cranfield0344', 'cranfield0345', 'cranfield0346', 'cranfield0347', 'cranfield0348', 'cranfield0349', 'cranfield0350', 'cranfield0351', 'cranfield0352', 'cranfield0353', 'cranfield0354', 'cranfield0355', 'cranfield0356', 'cranfield0357', 'cranfield0358', 'cranfield0359', 'cranfield0360', 'cranfield0361', 'cranfield0362', 'cranfield0363', 'cranfield0364', 'cranfield0365', 'cranfield0366', 'cranfield0367', 'cranfield0368', 'cranfield0369', 'cranfield0370', 'cranfield0371', 'cranfield0372', 'cranfield0373', 'cranfield0374', 'cranfield0375', 'cranfield0376', 'cranfield0377', 'cranfield0378', 'cranfield0379', 'cranfield0380', 'cranfield0381', 'cranfield0382', 'cranfield0383', 'cranfield0384', 'cranfield0385', 'cranfield0386', 'cranfield0387', 'cranfield0388', 'cranfield0389', 'cranfield0390', 'cranfield0391', 'cranfield0392', 'cranfield0393', 'cranfield0394', 'cranfield0395', 'cranfield0396', 'cranfield0397', 'cranfield0398', 'cranfield0399', 'cranfield0400', 'cranfield0401', 'cranfield0402', 'cranfield0403', 'cranfield0404', 'cranfield0405', 'cranfield0406', 'cranfield0407', 'cranfield0408', 'cranfield0409', 'cranfield0410', 'cranfield0411', 'cranfield0412', 'cranfield0413', 'cranfield0414', 'cranfield0415', 'cranfield0416', 'cranfield0417', 'cranfield0418', 'cranfield0419', 'cranfield0420', 'cranfield0421', 'cranfield0422', 'cranfield0423', 'cranfield0424', 'cranfield0425', 'cranfield0426', 'cranfield0427', 'cranfield0428', 'cranfield0429', 'cranfield0430', 'cranfield0431', 'cranfield0432', 'cranfield0433', 'cranfield0434', 'cranfield0435', 'cranfield0436', 'cranfield0437', 'cranfield0438', 'cranfield0439', 'cranfield0440', 'cranfield0441', 'cranfield0442', 'cranfield0443', 'cranfield0444', 'cranfield0445', 'cranfield0446', 'cranfield0447', 'cranfield0448', 'cranfield0449', 'cranfield0450', 'cranfield0451', 'cranfield0452', 'cranfield0453', 'cranfield0454', 'cranfield0455', 'cranfield0456', 'cranfield0457', 'cranfield0458', 'cranfield0459', 'cranfield0460', 'cranfield0461', 'cranfield0462', 'cranfield0463', 'cranfield0464', 'cranfield0465', 'cranfield0466', 'cranfield0467', 'cranfield0468', 'cranfield0469', 'cranfield0470', 'cranfield0471', 'cranfield0472', 'cranfield0473', 'cranfield0474', 'cranfield0475', 'cranfield0476', 'cranfield0477', 'cranfield0478', 'cranfield0479', 'cranfield0480', 'cranfield0481', 'cranfield0482', 'cranfield0483', 'cranfield0484', 'cranfield0485', 'cranfield0486', 'cranfield0487', 'cranfield0488', 'cranfield0489', 'cranfield0490', 'cranfield0491', 'cranfield0492', 'cranfield0493', 'cranfield0494', 'cranfield0495', 'cranfield0496', 'cranfield0497', 'cranfield0498', 'cranfield0499', 'cranfield0500', 'cranfield0501', 'cranfield0502', 'cranfield0503', 'cranfield0504', 'cranfield0505', 'cranfield0506', 'cranfield0507', 'cranfield0508', 'cranfield0509', 'cranfield0510', 'cranfield0511', 'cranfield0512', 'cranfield0513', 'cranfield0514', 'cranfield0515', 'cranfield0516', 'cranfield0517', 'cranfield0518', 'cranfield0519', 'cranfield0520', 'cranfield0521', 'cranfield0522', 'cranfield0523', 'cranfield0524', 'cranfield0525', 'cranfield0526', 'cranfield0527', 'cranfield0528', 'cranfield0529', 'cranfield0530', 'cranfield0531', 'cranfield0532', 'cranfield0533', 'cranfield0534', 'cranfield0535', 'cranfield0536', 'cranfield0537', 'cranfield0538', 'cranfield0539', 'cranfield0540', 'cranfield0541', 'cranfield0542', 'cranfield0543', 'cranfield0544', 'cranfield0545', 'cranfield0546', 'cranfield0547', 'cranfield0548', 'cranfield0549', 'cranfield0550', 'cranfield0551', 'cranfield0552', 'cranfield0553', 'cranfield0554', 'cranfield0555', 'cranfield0556', 'cranfield0557', 'cranfield0558', 'cranfield0559', 'cranfield0560', 'cranfield0561', 'cranfield0562', 'cranfield0563', 'cranfield0564', 'cranfield0565', 'cranfield0566', 'cranfield0567', 'cranfield0568', 'cranfield0569', 'cranfield0570', 'cranfield0571', 'cranfield0572', 'cranfield0573', 'cranfield0574', 'cranfield0575', 'cranfield0576', 'cranfield0577', 'cranfield0578', 'cranfield0579', 'cranfield0580', 'cranfield0581', 'cranfield0582', 'cranfield0583', 'cranfield0584', 'cranfield0585', 'cranfield0586', 'cranfield0587', 'cranfield0588', 'cranfield0589', 'cranfield0590', 'cranfield0591', 'cranfield0592', 'cranfield0593', 'cranfield0594', 'cranfield0595', 'cranfield0596', 'cranfield0597', 'cranfield0598', 'cranfield0599', 'cranfield0600', 'cranfield0601', 'cranfield0602', 'cranfield0603', 'cranfield0604', 'cranfield0605', 'cranfield0606', 'cranfield0607', 'cranfield0608', 'cranfield0609', 'cranfield0610', 'cranfield0611', 'cranfield0612', 'cranfield0613', 'cranfield0614', 'cranfield0615', 'cranfield0616', 'cranfield0617', 'cranfield0618', 'cranfield0619', 'cranfield0620', 'cranfield0621', 'cranfield0622', 'cranfield0623', 'cranfield0624', 'cranfield0625', 'cranfield0626', 'cranfield0627', 'cranfield0628', 'cranfield0629', 'cranfield0630', 'cranfield0631', 'cranfield0632', 'cranfield0633', 'cranfield0634', 'cranfield0635', 'cranfield0636', 'cranfield0637', 'cranfield0638', 'cranfield0639', 'cranfield0640', 'cranfield0641', 'cranfield0642', 'cranfield0643', 'cranfield0644', 'cranfield0645', 'cranfield0646', 'cranfield0647', 'cranfield0648', 'cranfield0649', 'cranfield0650', 'cranfield0651', 'cranfield0652', 'cranfield0653', 'cranfield0654', 'cranfield0655', 'cranfield0656', 'cranfield0657', 'cranfield0658', 'cranfield0659', 'cranfield0660', 'cranfield0661', 'cranfield0662', 'cranfield0663', 'cranfield0664', 'cranfield0665', 'cranfield0666', 'cranfield0667', 'cranfield0668', 'cranfield0669', 'cranfield0670', 'cranfield0671', 'cranfield0672', 'cranfield0673', 'cranfield0674', 'cranfield0675', 'cranfield0676', 'cranfield0677', 'cranfield0678', 'cranfield0679', 'cranfield0680', 'cranfield0681', 'cranfield0682', 'cranfield0683', 'cranfield0684', 'cranfield0685', 'cranfield0686', 'cranfield0687', 'cranfield0688', 'cranfield0689', 'cranfield0690', 'cranfield0691', 'cranfield0692', 'cranfield0693', 'cranfield0694', 'cranfield0695', 'cranfield0696', 'cranfield0697', 'cranfield0698', 'cranfield0699', 'cranfield0700', 'cranfield0701', 'cranfield0702', 'cranfield0703', 'cranfield0704', 'cranfield0705', 'cranfield0706', 'cranfield0707', 'cranfield0708', 'cranfield0709', 'cranfield0710', 'cranfield0711', 'cranfield0712', 'cranfield0713', 'cranfield0714', 'cranfield0715', 'cranfield0716', 'cranfield0717', 'cranfield0718', 'cranfield0719', 'cranfield0720', 'cranfield0721', 'cranfield0722', 'cranfield0723', 'cranfield0724', 'cranfield0725', 'cranfield0726', 'cranfield0727', 'cranfield0728', 'cranfield0729', 'cranfield0730', 'cranfield0731', 'cranfield0732', 'cranfield0733', 'cranfield0734', 'cranfield0735', 'cranfield0736', 'cranfield0737', 'cranfield0738', 'cranfield0739', 'cranfield0740', 'cranfield0741', 'cranfield0742', 'cranfield0743', 'cranfield0744', 'cranfield0745', 'cranfield0746', 'cranfield0747', 'cranfield0748', 'cranfield0749', 'cranfield0750', 'cranfield0751', 'cranfield0752', 'cranfield0753', 'cranfield0754', 'cranfield0755', 'cranfield0756', 'cranfield0757', 'cranfield0758', 'cranfield0759', 'cranfield0760', 'cranfield0761', 'cranfield0762', 'cranfield0763', 'cranfield0764', 'cranfield0765', 'cranfield0766', 'cranfield0767', 'cranfield0768', 'cranfield0769', 'cranfield0770', 'cranfield0771', 'cranfield0772', 'cranfield0773', 'cranfield0774', 'cranfield0775', 'cranfield0776', 'cranfield0777', 'cranfield0778', 'cranfield0779', 'cranfield0780', 'cranfield0781', 'cranfield0782', 'cranfield0783', 'cranfield0784', 'cranfield0785', 'cranfield0786', 'cranfield0787', 'cranfield0788', 'cranfield0789', 'cranfield0790', 'cranfield0791', 'cranfield0792', 'cranfield0793', 'cranfield0794', 'cranfield0795', 'cranfield0796', 'cranfield0797', 'cranfield0798', 'cranfield0799', 'cranfield0800', 'cranfield0801', 'cranfield0802', 'cranfield0803', 'cranfield0804', 'cranfield0805', 'cranfield0806', 'cranfield0807', 'cranfield0808', 'cranfield0809', 'cranfield0810', 'cranfield0811', 'cranfield0812', 'cranfield0813', 'cranfield0814', 'cranfield0815', 'cranfield0816', 'cranfield0817', 'cranfield0818', 'cranfield0819', 'cranfield0820', 'cranfield0821', 'cranfield0822', 'cranfield0823', 'cranfield0824', 'cranfield0825', 'cranfield0826', 'cranfield0827', 'cranfield0828', 'cranfield0829', 'cranfield0830', 'cranfield0831', 'cranfield0832', 'cranfield0833', 'cranfield0834', 'cranfield0835', 'cranfield0836', 'cranfield0837', 'cranfield0838', 'cranfield0839', 'cranfield0840', 'cranfield0841', 'cranfield0842', 'cranfield0843', 'cranfield0844', 'cranfield0845', 'cranfield0846', 'cranfield0847', 'cranfield0848', 'cranfield0849', 'cranfield0850', 'cranfield0851', 'cranfield0852', 'cranfield0853', 'cranfield0854', 'cranfield0855', 'cranfield0856', 'cranfield0857', 'cranfield0858', 'cranfield0859', 'cranfield0860', 'cranfield0861', 'cranfield0862', 'cranfield0863', 'cranfield0864', 'cranfield0865', 'cranfield0866', 'cranfield0867', 'cranfield0868', 'cranfield0869', 'cranfield0870', 'cranfield0871', 'cranfield0872', 'cranfield0873', 'cranfield0874', 'cranfield0875', 'cranfield0876', 'cranfield0877', 'cranfield0878', 'cranfield0879', 'cranfield0880', 'cranfield0881', 'cranfield0882', 'cranfield0883', 'cranfield0884', 'cranfield0885', 'cranfield0886', 'cranfield0887', 'cranfield0888', 'cranfield0889', 'cranfield0890', 'cranfield0891', 'cranfield0892', 'cranfield0893', 'cranfield0894', 'cranfield0895', 'cranfield0896', 'cranfield0897', 'cranfield0898', 'cranfield0899', 'cranfield0900', 'cranfield0901', 'cranfield0902', 'cranfield0903', 'cranfield0904', 'cranfield0905', 'cranfield0906', 'cranfield0907', 'cranfield0908', 'cranfield0909', 'cranfield0910', 'cranfield0911', 'cranfield0912', 'cranfield0913', 'cranfield0914', 'cranfield0915', 'cranfield0916', 'cranfield0917', 'cranfield0918', 'cranfield0919', 'cranfield0920', 'cranfield0921', 'cranfield0922', 'cranfield0923', 'cranfield0924', 'cranfield0925', 'cranfield0926', 'cranfield0927', 'cranfield0928', 'cranfield0929', 'cranfield0930', 'cranfield0931', 'cranfield0932', 'cranfield0933', 'cranfield0934', 'cranfield0935', 'cranfield0936', 'cranfield0937', 'cranfield0938', 'cranfield0939', 'cranfield0940', 'cranfield0941', 'cranfield0942', 'cranfield0943', 'cranfield0944', 'cranfield0945', 'cranfield0946', 'cranfield0947', 'cranfield0948', 'cranfield0949', 'cranfield0950', 'cranfield0951', 'cranfield0952', 'cranfield0953', 'cranfield0954', 'cranfield0955', 'cranfield0956', 'cranfield0957', 'cranfield0958', 'cranfield0959', 'cranfield0960', 'cranfield0961', 'cranfield0962', 'cranfield0963', 'cranfield0964', 'cranfield0965', 'cranfield0966', 'cranfield0967', 'cranfield0968', 'cranfield0969', 'cranfield0970', 'cranfield0971', 'cranfield0972', 'cranfield0973', 'cranfield0974', 'cranfield0975', 'cranfield0976', 'cranfield0977', 'cranfield0978', 'cranfield0979', 'cranfield0980', 'cranfield0981', 'cranfield0982', 'cranfield0983', 'cranfield0984', 'cranfield0985', 'cranfield0986', 'cranfield0987', 'cranfield0988', 'cranfield0989', 'cranfield0990', 'cranfield0991', 'cranfield0992', 'cranfield0993', 'cranfield0994', 'cranfield0995', 'cranfield0996', 'cranfield0997', 'cranfield0998', 'cranfield0999', 'cranfield1000', 'cranfield1001', 'cranfield1002', 'cranfield1003', 'cranfield1004', 'cranfield1005', 'cranfield1006', 'cranfield1007', 'cranfield1008', 'cranfield1009', 'cranfield1010', 'cranfield1011', 'cranfield1012', 'cranfield1013', 'cranfield1014', 'cranfield1015', 'cranfield1016', 'cranfield1017', 'cranfield1018', 'cranfield1019', 'cranfield1020', 'cranfield1021', 'cranfield1022', 'cranfield1023', 'cranfield1024', 'cranfield1025', 'cranfield1026', 'cranfield1027', 'cranfield1028', 'cranfield1029', 'cranfield1030', 'cranfield1031', 'cranfield1032', 'cranfield1033', 'cranfield1034', 'cranfield1035', 'cranfield1036', 'cranfield1037', 'cranfield1038', 'cranfield1039', 'cranfield1040', 'cranfield1041', 'cranfield1042', 'cranfield1043', 'cranfield1044', 'cranfield1045', 'cranfield1046', 'cranfield1047', 'cranfield1048', 'cranfield1049', 'cranfield1050', 'cranfield1051', 'cranfield1052', 'cranfield1053', 'cranfield1054', 'cranfield1055', 'cranfield1056', 'cranfield1057', 'cranfield1058', 'cranfield1059', 'cranfield1060', 'cranfield1061', 'cranfield1062', 'cranfield1063', 'cranfield1064', 'cranfield1065', 'cranfield1066', 'cranfield1067', 'cranfield1068', 'cranfield1069', 'cranfield1070', 'cranfield1071', 'cranfield1072', 'cranfield1073', 'cranfield1074', 'cranfield1075', 'cranfield1076', 'cranfield1077', 'cranfield1078', 'cranfield1079', 'cranfield1080', 'cranfield1081', 'cranfield1082', 'cranfield1083', 'cranfield1084', 'cranfield1085', 'cranfield1086', 'cranfield1087', 'cranfield1088', 'cranfield1089', 'cranfield1090', 'cranfield1091', 'cranfield1092', 'cranfield1093', 'cranfield1094', 'cranfield1095', 'cranfield1096', 'cranfield1097', 'cranfield1098', 'cranfield1099', 'cranfield1100', 'cranfield1101', 'cranfield1102', 'cranfield1103', 'cranfield1104', 'cranfield1105', 'cranfield1106', 'cranfield1107', 'cranfield1108', 'cranfield1109', 'cranfield1110', 'cranfield1111', 'cranfield1112', 'cranfield1113', 'cranfield1114', 'cranfield1115', 'cranfield1116', 'cranfield1117', 'cranfield1118', 'cranfield1119', 'cranfield1120', 'cranfield1121', 'cranfield1122', 'cranfield1123', 'cranfield1124', 'cranfield1125', 'cranfield1126', 'cranfield1127', 'cranfield1128', 'cranfield1129', 'cranfield1130', 'cranfield1131', 'cranfield1132', 'cranfield1133', 'cranfield1134', 'cranfield1135', 'cranfield1136', 'cranfield1137', 'cranfield1138', 'cranfield1139', 'cranfield1140', 'cranfield1141', 'cranfield1142', 'cranfield1143', 'cranfield1144', 'cranfield1145', 'cranfield1146', 'cranfield1147', 'cranfield1148', 'cranfield1149', 'cranfield1150', 'cranfield1151', 'cranfield1152', 'cranfield1153', 'cranfield1154', 'cranfield1155', 'cranfield1156', 'cranfield1157', 'cranfield1158', 'cranfield1159', 'cranfield1160', 'cranfield1161', 'cranfield1162', 'cranfield1163', 'cranfield1164', 'cranfield1165', 'cranfield1166', 'cranfield1167', 'cranfield1168', 'cranfield1169', 'cranfield1170', 'cranfield1171', 'cranfield1172', 'cranfield1173', 'cranfield1174', 'cranfield1175', 'cranfield1176', 'cranfield1177', 'cranfield1178', 'cranfield1179', 'cranfield1180', 'cranfield1181', 'cranfield1182', 'cranfield1183', 'cranfield1184', 'cranfield1185', 'cranfield1186', 'cranfield1187', 'cranfield1188', 'cranfield1189', 'cranfield1190', 'cranfield1191', 'cranfield1192', 'cranfield1193', 'cranfield1194', 'cranfield1195', 'cranfield1196', 'cranfield1197', 'cranfield1198', 'cranfield1199', 'cranfield1200', 'cranfield1201', 'cranfield1202', 'cranfield1203', 'cranfield1204', 'cranfield1205', 'cranfield1206', 'cranfield1207', 'cranfield1208', 'cranfield1209', 'cranfield1210', 'cranfield1211', 'cranfield1212', 'cranfield1213', 'cranfield1214', 'cranfield1215', 'cranfield1216', 'cranfield1217', 'cranfield1218', 'cranfield1219', 'cranfield1220', 'cranfield1221', 'cranfield1222', 'cranfield1223', 'cranfield1224', 'cranfield1225', 'cranfield1226', 'cranfield1227', 'cranfield1228', 'cranfield1229', 'cranfield1230', 'cranfield1231', 'cranfield1232', 'cranfield1233', 'cranfield1234', 'cranfield1235', 'cranfield1236', 'cranfield1237', 'cranfield1238', 'cranfield1239', 'cranfield1240', 'cranfield1241', 'cranfield1242', 'cranfield1243', 'cranfield1244', 'cranfield1245', 'cranfield1246', 'cranfield1247', 'cranfield1248', 'cranfield1249', 'cranfield1250', 'cranfield1251', 'cranfield1252', 'cranfield1253', 'cranfield1254', 'cranfield1255', 'cranfield1256', 'cranfield1257', 'cranfield1258', 'cranfield1259', 'cranfield1260', 'cranfield1261', 'cranfield1262', 'cranfield1263', 'cranfield1264', 'cranfield1265', 'cranfield1266', 'cranfield1267', 'cranfield1268', 'cranfield1269', 'cranfield1270', 'cranfield1271', 'cranfield1272', 'cranfield1273', 'cranfield1274', 'cranfield1275', 'cranfield1276', 'cranfield1277', 'cranfield1278', 'cranfield1279', 'cranfield1280', 'cranfield1281', 'cranfield1282', 'cranfield1283', 'cranfield1284', 'cranfield1285', 'cranfield1286', 'cranfield1287', 'cranfield1288', 'cranfield1289', 'cranfield1290', 'cranfield1291', 'cranfield1292', 'cranfield1293', 'cranfield1294', 'cranfield1295', 'cranfield1296', 'cranfield1297', 'cranfield1298', 'cranfield1299', 'cranfield1300', 'cranfield1301', 'cranfield1302', 'cranfield1303', 'cranfield1304', 'cranfield1305', 'cranfield1306', 'cranfield1307', 'cranfield1308', 'cranfield1309', 'cranfield1310', 'cranfield1311', 'cranfield1312', 'cranfield1313', 'cranfield1314', 'cranfield1315', 'cranfield1316', 'cranfield1317', 'cranfield1318', 'cranfield1319', 'cranfield1320', 'cranfield1321', 'cranfield1322', 'cranfield1323', 'cranfield1324', 'cranfield1325', 'cranfield1326', 'cranfield1327', 'cranfield1328', 'cranfield1329', 'cranfield1330', 'cranfield1331', 'cranfield1332', 'cranfield1333', 'cranfield1334', 'cranfield1335', 'cranfield1336', 'cranfield1337', 'cranfield1338', 'cranfield1339', 'cranfield1340', 'cranfield1341', 'cranfield1342', 'cranfield1343', 'cranfield1344', 'cranfield1345', 'cranfield1346', 'cranfield1347', 'cranfield1348', 'cranfield1349', 'cranfield1350', 'cranfield1351', 'cranfield1352', 'cranfield1353', 'cranfield1354', 'cranfield1355', 'cranfield1356', 'cranfield1357', 'cranfield1358', 'cranfield1359', 'cranfield1360', 'cranfield1361', 'cranfield1362', 'cranfield1363', 'cranfield1364', 'cranfield1365', 'cranfield1366', 'cranfield1367', 'cranfield1368', 'cranfield1369', 'cranfield1370', 'cranfield1371', 'cranfield1372', 'cranfield1373', 'cranfield1374', 'cranfield1375', 'cranfield1376', 'cranfield1377', 'cranfield1378', 'cranfield1379', 'cranfield1380', 'cranfield1381', 'cranfield1382', 'cranfield1383', 'cranfield1384', 'cranfield1385', 'cranfield1386', 'cranfield1387', 'cranfield1388', 'cranfield1389', 'cranfield1390', 'cranfield1391', 'cranfield1392', 'cranfield1393', 'cranfield1394', 'cranfield1395', 'cranfield1396', 'cranfield1397', 'cranfield1398', 'cranfield1399', 'cranfield1400']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(filenames))\n",
        "print(len(filenames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfQ6Yu20ikMT",
        "outputId": "b5206d15-8ecd-499c-958a-19e85d00c094"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First 5 Documents"
      ],
      "metadata": {
        "id": "Y_nW0cj02aaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 5):\n",
        "  outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  doc = outfile.read()\n",
        "  print(\"------------\")\n",
        "  print(\"Document: \"+str(i+1))\n",
        "  print(\"------------\")\n",
        "  print(doc)\n",
        "  print('-----------------------------------------------------------------')\n",
        "  print('')\n",
        "  outfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UsRuAhbiwlI",
        "outputId": "0be494b3-26af-4fec-d325-202cd66c8402"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "Document: 1\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "1\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "brenckman,m.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 25, 1958, 324.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 2\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "2\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "ting-yili\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "department of aeronautical engineering, rensselaer polytechnic\n",
            "institute\n",
            "troy, n.y.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 3\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "3\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "m. b. glauert\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "department of mathematics, university of manchester, manchester,\n",
            "england\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 4\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "4\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "yen,k.t.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 22, 1955, 728.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 5\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "5\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "wasserman,b.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 24, 1957, 924.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting content between TITLE and TEXT tag from all documents."
      ],
      "metadata": {
        "id": "B004slWy2oLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(filenames)):\n",
        "    outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "    doc = outfile.read()\n",
        "\n",
        "    # initializing string\n",
        "    test_str = doc\n",
        "\n",
        "    # initializing tags\n",
        "    tag1 = \"TITLE\"\n",
        "    tag2 = \"TEXT\"\n",
        "\n",
        "    # regex to extract required strings\n",
        "    reg_str1 = \"<\"+tag1+\">(.*?)</\"+tag1+\">\"\n",
        "    res1 = re.findall(reg_str1, test_str, re.DOTALL)\n",
        "\n",
        "    reg_str2 = \"<\"+tag2+\">(.*?)</\"+tag2+\">\"\n",
        "    res2 = re.findall(reg_str2, test_str, re.DOTALL)\n",
        "\n",
        "    #Combining contents of TITLE and TEXT\n",
        "    res = res1+res2\n",
        "    \n",
        "    s = res\n",
        "\n",
        "    # using list comprehension\n",
        "    listToStr = ' '.join([str(elem) for elem in s])\n",
        "\n",
        "    writeFile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'w')\n",
        "    L = listToStr\n",
        "\n",
        "    writeFile.write(L)\n",
        "    writeFile.close()\n",
        "\n",
        "    outfile.close()"
      ],
      "metadata": {
        "id": "zyTW1S39mbe3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First 5 Documents after extracting contents between TITLE and TEXT tag."
      ],
      "metadata": {
        "id": "hwG6yHOS26Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 5):\n",
        "  outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  doc = outfile.read()\n",
        "  print(\"------------\")\n",
        "  print(\"Document: \"+str(i+1))\n",
        "  print(\"------------\")\n",
        "  print(doc)\n",
        "  print('-----------------------------------------------------------------')\n",
        "  print('')\n",
        "  outfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDU8pbuGs0Fq",
        "outputId": "b1fdec82-bfe6-44f0-c6ae-48dd7784a78a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "Document: 1\n",
            "------------\n",
            "\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            " \n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 2\n",
            "------------\n",
            "\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            " \n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 3\n",
            "------------\n",
            "\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            " \n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 4\n",
            "------------\n",
            "\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            " \n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 5\n",
            "------------\n",
            "\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            " \n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (ii) Preprocessing"
      ],
      "metadata": {
        "id": "hR_mu5eB3R9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Lowercase the text"
      ],
      "metadata": {
        "id": "Wb8YvKiy3a4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newfiles=[]\n",
        "for i in range(len(filenames)):\n",
        "  fileind=open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  filedata=fileind.read()\n",
        "  filedata=filedata.lower()\n",
        "  newfiles.append(filedata)\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj39fCHGs0De",
        "outputId": "2c3636e0-e109-45f9-a6e1-468a79763b9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            " \n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            " \n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            " \n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            " \n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            " \n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwN-GsEZvycR",
        "outputId": "98ae0fe8-eb4b-479b-e20c-ba89d242aaa9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Perform tokenization"
      ],
      "metadata": {
        "id": "qWubxSm33gUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  filedata=word_tokenize(filedata)\n",
        "  newfiles[i] = filedata\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4Bw0AHwvgtS",
        "outputId": "12e8ee85-8dce-4199-c1ea-ed2fef47c0d2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'of', 'the', 'aerodynamics', 'of', 'a', 'wing', 'in', 'a', 'slipstream', '.', 'an', 'experimental', 'study', 'of', 'a', 'wing', 'in', 'a', 'propeller', 'slipstream', 'was', 'made', 'in', 'order', 'to', 'determine', 'the', 'spanwise', 'distribution', 'of', 'the', 'lift', 'increase', 'due', 'to', 'slipstream', 'at', 'different', 'angles', 'of', 'attack', 'of', 'the', 'wing', 'and', 'at', 'different', 'free', 'stream', 'to', 'slipstream', 'velocity', 'ratios', '.', 'the', 'results', 'were', 'intended', 'in', 'part', 'as', 'an', 'evaluation', 'basis', 'for', 'different', 'theoretical', 'treatments', 'of', 'this', 'problem', '.', 'the', 'comparative', 'span', 'loading', 'curves', ',', 'together', 'with', 'supporting', 'evidence', ',', 'showed', 'that', 'a', 'substantial', 'part', 'of', 'the', 'lift', 'increment', 'produced', 'by', 'the', 'slipstream', 'was', 'due', 'to', 'a', '/destalling/', 'or', 'boundary-layer-control', 'effect', '.', 'the', 'integrated', 'remaining', 'lift', 'increment', ',', 'after', 'subtracting', 'this', 'destalling', 'lift', ',', 'was', 'found', 'to', 'agree', 'well', 'with', 'a', 'potential', 'flow', 'theory', '.', 'an', 'empirical', 'evaluation', 'of', 'the', 'destalling', 'effects', 'was', 'made', 'for', 'the', 'specific', 'configuration', 'of', 'the', 'experiment', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', 'in', 'an', 'incompressible', 'fluid', 'of', 'small', 'viscosity', '.', 'in', 'the', 'study', 'of', 'high-speed', 'viscous', 'flow', 'past', 'a', 'two-dimensional', 'body', 'it', 'is', 'usually', 'necessary', 'to', 'consider', 'a', 'curved', 'shock', 'wave', 'emitting', 'from', 'the', 'nose', 'or', 'leading', 'edge', 'of', 'the', 'body', '.', 'consequently', ',', 'there', 'exists', 'an', 'inviscid', 'rotational', 'flow', 'region', 'between', 'the', 'shock', 'wave', 'and', 'the', 'boundary', 'layer', '.', 'such', 'a', 'situation', 'arises', ',', 'for', 'instance', ',', 'in', 'the', 'study', 'of', 'the', 'hypersonic', 'viscous', 'flow', 'past', 'a', 'flat', 'plate', '.', 'the', 'situation', 'is', 'somewhat', 'different', 'from', 'prandtl', \"'s\", 'classical', 'boundary-layer', 'problem', '.', 'in', \"prandtl's\", 'original', 'problem', 'the', 'inviscid', 'free', 'stream', 'outside', 'the', 'boundary', 'layer', 'is', 'irrotational', 'while', 'in', 'a', 'hypersonic', 'boundary-layer', 'problem', 'the', 'inviscid', 'free', 'stream', 'must', 'be', 'considered', 'as', 'rotational', '.', 'the', 'possible', 'effects', 'of', 'vorticity', 'have', 'been', 'recently', 'discussed', 'by', 'ferri', 'and', 'libby', '.', 'in', 'the', 'present', 'paper', ',', 'the', 'simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', 'in', 'a', 'fluid', 'of', 'small', 'viscosity', 'is', 'investigated', '.', 'it', 'can', 'be', 'shown', 'that', 'this', 'problem', 'can', 'again', 'be', 'treated', 'by', 'the', 'boundary-layer', 'approximation', ',', 'the', 'only', 'novel', 'feature', 'being', 'that', 'the', 'free', 'stream', 'has', 'a', 'constant', 'vorticity', '.', 'the', 'discussion', 'here', 'is', 'restricted', 'to', 'two-dimensional', 'incompressible', 'steady', 'flow', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['the', 'boundary', 'layer', 'in', 'simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', '.', 'the', 'boundary-layer', 'equations', 'are', 'presented', 'for', 'steady', 'incompressible', 'flow', 'with', 'no', 'pressure', 'gradient', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'of', 'the', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'for', 'a', 'plate', 'in', 'shear', 'flow', '.', 'the', 'two-dimensional', 'steady', 'boundary-layer', 'problem', 'for', 'a', 'flat', 'plate', 'in', 'a', 'shear', 'flow', 'of', 'incompressible', 'fluid', 'is', 'considered', '.', 'solutions', 'for', 'the', 'boundarylayer', 'thickness', ',', 'skin', 'friction', ',', 'and', 'the', 'velocity', 'distribution', 'in', 'the', 'boundary', 'layer', 'are', 'obtained', 'by', 'the', 'karman-pohlhausen', 'technique', '.', 'comparison', 'with', 'the', 'boundary', 'layer', 'of', 'a', 'uniform', 'flow', 'has', 'also', 'been', 'made', 'to', 'show', 'the', 'effect', 'of', 'vorticity', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['one-dimensional', 'transient', 'heat', 'conduction', 'into', 'a', 'double-layer', 'slab', 'subjected', 'to', 'a', 'linear', 'heat', 'input', 'for', 'a', 'small', 'time', 'internal', '.', 'analytic', 'solutions', 'are', 'presented', 'for', 'the', 'transient', 'heat', 'conduction', 'in', 'composite', 'slabs', 'exposed', 'at', 'one', 'surface', 'to', 'a', 'triangular', 'heat', 'rate', '.', 'this', 'type', 'of', 'heating', 'rate', 'may', 'occur', ',', 'for', 'example', ',', 'during', 'aerodynamic', 'heating', '.']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "JHqdAX-JvghM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Remove stopwords"
      ],
      "metadata": {
        "id": "vGceyDFv3k3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_stopwords = [x for x in filedata if x not in stopwords_set]\n",
        "  newfiles[i] = tokens_sans_stopwords\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRJSvynOvgWx",
        "outputId": "a07d1451-0d35-4be2-99df-2e61dff00fc9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', '.', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', '.', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', '.', 'comparative', 'span', 'loading', 'curves', ',', 'together', 'supporting', 'evidence', ',', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', '/destalling/', 'boundary-layer-control', 'effect', '.', 'integrated', 'remaining', 'lift', 'increment', ',', 'subtracting', 'destalling', 'lift', ',', 'found', 'agree', 'well', 'potential', 'flow', 'theory', '.', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', '.', 'study', 'high-speed', 'viscous', 'flow', 'past', 'two-dimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', '.', 'consequently', ',', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', '.', 'situation', 'arises', ',', 'instance', ',', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', '.', 'situation', 'somewhat', 'different', 'prandtl', \"'s\", 'classical', 'boundary-layer', 'problem', '.', \"prandtl's\", 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundary-layer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', '.', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', '.', 'present', 'paper', ',', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', '.', 'shown', 'problem', 'treated', 'boundary-layer', 'approximation', ',', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', '.', 'discussion', 'restricted', 'two-dimensional', 'incompressible', 'steady', 'flow', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', '.', 'boundary-layer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', '.', 'two-dimensional', 'steady', 'boundary-layer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', '.', 'solutions', 'boundarylayer', 'thickness', ',', 'skin', 'friction', ',', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karman-pohlhausen', 'technique', '.', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['one-dimensional', 'transient', 'heat', 'conduction', 'double-layer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', '.', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', '.', 'type', 'heating', 'rate', 'may', 'occur', ',', 'example', ',', 'aerodynamic', 'heating', '.']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Remove punctuations"
      ],
      "metadata": {
        "id": "-litrUFw3ocO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in filedata]\n",
        "  newfiles[i] = tokens_sans_punctuation\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X00UdJZpszv0",
        "outputId": "af73fcd2-81cc-4f3d-f491-b4909a28ea5b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', '', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', '', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', '', 'comparative', 'span', 'loading', 'curves', '', 'together', 'supporting', 'evidence', '', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', 'destalling', 'boundarylayercontrol', 'effect', '', 'integrated', 'remaining', 'lift', 'increment', '', 'subtracting', 'destalling', 'lift', '', 'found', 'agree', 'well', 'potential', 'flow', 'theory', '', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', '', 'study', 'highspeed', 'viscous', 'flow', 'past', 'twodimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', '', 'consequently', '', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', '', 'situation', 'arises', '', 'instance', '', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', '', 'situation', 'somewhat', 'different', 'prandtl', 's', 'classical', 'boundarylayer', 'problem', '', 'prandtls', 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundarylayer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', '', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', '', 'present', 'paper', '', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', '', 'shown', 'problem', 'treated', 'boundarylayer', 'approximation', '', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', '', 'discussion', 'restricted', 'twodimensional', 'incompressible', 'steady', 'flow', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', '', 'boundarylayer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', '', 'twodimensional', 'steady', 'boundarylayer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', '', 'solutions', 'boundarylayer', 'thickness', '', 'skin', 'friction', '', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karmanpohlhausen', 'technique', '', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['onedimensional', 'transient', 'heat', 'conduction', 'doublelayer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', '', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', '', 'type', 'heating', 'rate', 'may', 'occur', '', 'example', '', 'aerodynamic', 'heating', '']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Remove blank space tokens"
      ],
      "metadata": {
        "id": "3FMLU0E-3spS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lent = 0\n",
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_blank_space = [x for x in filedata if x!='']\n",
        "  newfiles[i] = tokens_sans_blank_space\n",
        "  lent += len(newfiles[i])\n",
        "print(len(newfiles))\n",
        "print(lent)\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwvbE8aIszs4",
        "outputId": "440af0f0-c964-4c73-ece5-1f4a940d5cc8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "127377\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', 'comparative', 'span', 'loading', 'curves', 'together', 'supporting', 'evidence', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', 'destalling', 'boundarylayercontrol', 'effect', 'integrated', 'remaining', 'lift', 'increment', 'subtracting', 'destalling', 'lift', 'found', 'agree', 'well', 'potential', 'flow', 'theory', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', 'study', 'highspeed', 'viscous', 'flow', 'past', 'twodimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', 'consequently', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', 'situation', 'arises', 'instance', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', 'situation', 'somewhat', 'different', 'prandtl', 's', 'classical', 'boundarylayer', 'problem', 'prandtls', 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundarylayer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', 'present', 'paper', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', 'shown', 'problem', 'treated', 'boundarylayer', 'approximation', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', 'discussion', 'restricted', 'twodimensional', 'incompressible', 'steady', 'flow']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'boundarylayer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', 'twodimensional', 'steady', 'boundarylayer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', 'solutions', 'boundarylayer', 'thickness', 'skin', 'friction', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karmanpohlhausen', 'technique', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['onedimensional', 'transient', 'heat', 'conduction', 'doublelayer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', 'type', 'heating', 'rate', 'may', 'occur', 'example', 'aerodynamic', 'heating']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b0yA6yiFszq2"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JablNRzcszln"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 2 STARTS HERE\n"
      ],
      "metadata": {
        "id": "5v4VqJWSCHpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Boolean Queries"
      ],
      "metadata": {
        "id": "NRru76l0DWit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Create a unigram inverted index(from scratch; No library allowed) of the dataset obtained from Q1 (after preprocessing)."
      ],
      "metadata": {
        "id": "peVloS7fDbbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Use Python’s pickle module to save and load the unigram inverted index."
      ],
      "metadata": {
        "id": "v8oe0ZKQ4q9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_unigram_inverted_index(file_dictionary, stopwords_set):\n",
        "    # initialize unigram inverted index\n",
        "    unigram_inverted_index = SortedDict()\n",
        "    \n",
        "    # unigram inverted index\n",
        "    for doc_ID in range(len(file_dictionary)):\n",
        "        file = open(file_dictionary[doc_ID], 'r', encoding='utf-8', errors='ignore')\n",
        "        file_corpus = file.read()\n",
        "        file.close()\n",
        "        doc_tokens = preprocess(file_corpus, stopwords_set, 'doc')\n",
        "        for token in doc_tokens:\n",
        "            if token in unigram_inverted_index:\n",
        "                unigram_inverted_index[token][0] += 1   # stores doc_frequency\n",
        "                unigram_inverted_index[token][1].add(doc_ID)    # appending docs in which that term occurs\n",
        "            else:\n",
        "                unigram_inverted_index[token] = [1, SortedSet([doc_ID])]    # initializing if this token appears for first time\n",
        "    \n",
        "    # Storing unigram inverted index\n",
        "    uii_file = open('unigram_inverted_index_pickle_file', 'wb')\n",
        "    pickle.dump(unigram_inverted_index, uii_file)\n",
        "    uii_file.close()"
      ],
      "metadata": {
        "id": "ZADY2iouszjF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Provide support for the following operations: \n",
        "a. T1 AND T2<br>\n",
        "b. T1 AND NOT T2<br>\n",
        "c. T1 OR T2<br>\n",
        "d. T1 OR NOT T2"
      ],
      "metadata": {
        "id": "3Cz0lNlV42Cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AND"
      ],
      "metadata": {
        "id": "O1oj5ZZR5esI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xANDy(pos_list_1, pos_list_2):\n",
        "    result = SortedSet()\n",
        "    i,j = 0,0\n",
        "    num_of_operations = 0\n",
        "    while(i<len(pos_list_1) and j<len(pos_list_2)):\n",
        "        if pos_list_1[i]==pos_list_2[j]:\n",
        "            result.add(pos_list_1[i])\n",
        "            i+=1\n",
        "            j+=1\n",
        "        elif pos_list_1[i]<pos_list_2[j]:\n",
        "            i+=1\n",
        "        else:\n",
        "            j+=1\n",
        "        num_of_operations+=1\n",
        "    \n",
        "    return result, num_of_operations"
      ],
      "metadata": {
        "id": "tzreAL103-QI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR"
      ],
      "metadata": {
        "id": "mI2KNVBh5kiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xORy(pos_list_1, pos_list_2):\n",
        "    result = SortedSet()\n",
        "    i,j = 0,0\n",
        "    num_of_operations = 0\n",
        "    while(i<len(pos_list_1) and j<len(pos_list_2)):\n",
        "        if pos_list_1[i]==pos_list_2[j]:\n",
        "            result.add(pos_list_1[i])\n",
        "            i+=1\n",
        "            j+=1\n",
        "        elif pos_list_1[i]<pos_list_2[j]:\n",
        "            result.add(pos_list_1[i])\n",
        "            i+=1\n",
        "        else:\n",
        "            result.add(pos_list_2[j])\n",
        "            j+=1\n",
        "        num_of_operations+=1\n",
        "    \n",
        "    while(i<len(pos_list_1)):\n",
        "        result.add(pos_list_1[i])\n",
        "        i+=1\n",
        "    while(j<len(pos_list_2)):\n",
        "        result.add(pos_list_2[j])\n",
        "        j+=1\n",
        "    \n",
        "    return result, num_of_operations"
      ],
      "metadata": {
        "id": "CwkXBpjo5gCM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AND NOT"
      ],
      "metadata": {
        "id": "QScbMsi55rqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xANDNOTy(pos_list_1, pos_list_2):\n",
        "    result = SortedSet()\n",
        "    i,j = 0,0\n",
        "    num_of_operations = 0\n",
        "    while(i<len(pos_list_1) and j<len(pos_list_2)):\n",
        "        if pos_list_1[i]==pos_list_2[j]:\n",
        "            i+=1\n",
        "            j+=1\n",
        "        elif pos_list_1[i]<pos_list_2[j]:\n",
        "            result.add(pos_list_1[i])\n",
        "            i+=1\n",
        "        else:\n",
        "            j+=1\n",
        "        num_of_operations+=1\n",
        "    \n",
        "    while(i<len(pos_list_1)):\n",
        "        result.add(pos_list_1[i])\n",
        "        i+=1\n",
        "    \n",
        "    return result, num_of_operations"
      ],
      "metadata": {
        "id": "vUEbENl95ma7"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR NOT"
      ],
      "metadata": {
        "id": "twuRTZdb53gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xORNOTy(pos_list_1, pos_list_2, u_list):\n",
        "    num_of_not_operations = 0\n",
        "    i,j = 0,0\n",
        "    not_pos_list_2 = SortedSet()\n",
        "    while(i<len(pos_list_2) and j<len(u_list)):\n",
        "        if pos_list_2[i]<u_list[j]:\n",
        "            i+=1\n",
        "        elif pos_list_2[i]>u_list[j]:\n",
        "            not_pos_list_2.add(u_list[j])\n",
        "            j+=1\n",
        "        else:\n",
        "            i+=1\n",
        "            j+=1\n",
        "        num_of_not_operations+=1\n",
        "    while(j<len(u_list)):\n",
        "        not_pos_list_2.add(u_list[j])\n",
        "        j+=1\n",
        "    \n",
        "    result, ops = xORy(pos_list_1, not_pos_list_2)\n",
        "    return result, num_of_not_operations+ops"
      ],
      "metadata": {
        "id": "U2DQN7Et5ynp"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Queries should be generalized i.e., you should provide support for queries like T1 AND T2 OR T3 OR T4 ...."
      ],
      "metadata": {
        "id": "xd2b5rDC6eQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. You are also required to compute the minimum number of comparisons done to execute the query [only where merging is required]"
      ],
      "metadata": {
        "id": "8sAvW61c61fO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Input format: <br>\n",
        "a. The first line contains N denoting the number of queries to execute<br>\n",
        "b. The next 2N lines contain queries in the following format:<br>\n",
        "&ensp; i. Input sequence<br>\n",
        "&ensp; ii. Operations separated by comma"
      ],
      "metadata": {
        "id": "XBAFpN1W7ZTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Output Format:\n",
        "a. 4N lines consisting of the results in the following format:<br>\n",
        "&ensp;i. Query X<br>\n",
        "&ensp;ii. Number of documents retrieved for query X<br>\n",
        "&ensp;iii. Names of the documents retrieved for query X<br>\n",
        "&ensp;iv. Number of comparisons required for query X<br>"
      ],
      "metadata": {
        "id": "LOYk4XwI8Vli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Perform preprocessing steps (from Q1) on the input sequence as well."
      ],
      "metadata": {
        "id": "x7KgEgio7EfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # create set of stop words for preprocessing\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    \n",
        "    # Get List of Files in Dataset\n",
        "    list_of_files = getListOfFiles('/content/CSE508_Winter2023_Dataset')\n",
        "    \n",
        "    # create dictionary of file with docID (integer) as key and full_path of file as value\n",
        "    file_dictionary = create_file_dictionary(list_of_files)\n",
        "    \n",
        "    # create unigram inverted index once and then load pickle file afterwards\n",
        "    create_unigram_inverted_index(file_dictionary, stopwords_set)\n",
        "    \n",
        "    #Loading pre-processed files\n",
        "    uii_file = open('unigram_inverted_index_pickle_file', 'rb')\n",
        "    unigram_inverted_index = pickle.load(uii_file)\n",
        "    uii_file.close()\n",
        "    u_list = SortedSet()\n",
        "    for a in range(len(file_dictionary)):\n",
        "        u_list.add(a)\n",
        "    \n",
        "    N = int(input())\n",
        "    for q in range(N):\n",
        "        input_sentence = input()\n",
        "        input_operation_sequence = input()\n",
        "        sanitized_query = preprocess(input_sentence, stopwords_set, 'query')\n",
        "        sanitized_operation_sequence = []\n",
        "        for op_seq in input_operation_sequence.split(','):\n",
        "            sanitized_operation_sequence.append(op_seq.lower().strip())\n",
        "        print(sanitized_query)\n",
        "        print(sanitized_operation_sequence)\n",
        "        if(len(sanitized_query)!=len(sanitized_operation_sequence)+1):\n",
        "            print('invalid query')\n",
        "            continue    # will continue with next query \n",
        "        ptr1,ptr2 = 0,0\n",
        "        result = unigram_inverted_index[sanitized_query[ptr1]][1]\n",
        "        num_of_operations = 0\n",
        "        while(ptr2<len(sanitized_operation_sequence)):\n",
        "            if sanitized_operation_sequence[ptr2]=='and':\n",
        "                res, ops = xANDy(result, unigram_inverted_index[sanitized_query[ptr1+1]][1])\n",
        "            elif sanitized_operation_sequence[ptr2]=='or':\n",
        "                res, ops = xORy(result, unigram_inverted_index[sanitized_query[ptr1+1]][1])\n",
        "            elif sanitized_operation_sequence[ptr2]=='and not':\n",
        "                res, ops = xANDNOTy(result, unigram_inverted_index[sanitized_query[ptr1+1]][1])\n",
        "            elif sanitized_operation_sequence[ptr2]=='or not':\n",
        "                res, ops = xORNOTy(result, unigram_inverted_index[sanitized_query[ptr1+1]][1], u_list)\n",
        "            ptr1+=1\n",
        "            ptr2+=1\n",
        "            result = res\n",
        "            num_of_operations += ops\n",
        "        print('Number of documents matched: {}'.format(len(result)))\n",
        "        print('Number of comparisons required: {}'.format(num_of_operations))\n",
        "        print('Documents: \\n')\n",
        "        for res_doc in result:\n",
        "            print(file_dictionary[res_doc])"
      ],
      "metadata": {
        "id": "rjtOa6Yf5uk5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "uZ-_POQl6qbc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}