{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammed-Taasir/IR_2023-GROUP-14/blob/assignment-1/IR_Assignment_Question_3(A).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWAYXQ0oSqZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a43a4a34-4dbc-494e-d5d9-a0aaf69f1f0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/IR_Assignment_1_Dataset/CSE508_Winter2023_Dataset.zip' '/content/'\n",
        "!unzip 'CSE508_Winter2023_Dataset.zip' &> /dev/null\n",
        "!rm 'CSE508_Winter2023_Dataset.zip'"
      ],
      "metadata": {
        "id": "ASJxIYUHrJVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/CSE508_Winter2023_Dataset/'"
      ],
      "metadata": {
        "id": "0zfOelEUHL46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import nltk"
      ],
      "metadata": {
        "id": "PD1AsTJzSxYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sortedcontainers import SortedDict, SortedList, SortedSet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import PlaintextCorpusReader \n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "On3RRYHU765j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEtIpnnRTEDA",
        "outputId": "c3c7fa48-fe9e-40b8-eb3c-e825eff19f4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# * Defining Helper Functions"
      ],
      "metadata": {
        "id": "2KjMCCfaxY1_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Read from files"
      ],
      "metadata": {
        "id": "AAWglbrwxfUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def getListOfFiles(directory):\n",
        "\n",
        "    # Parameters: directory: type(string)        \n",
        "    # returns: list of all files in directory with the full path of file\n",
        "    \n",
        "    list_of_files = []\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "      fi = \"/content/CSE508_Winter2023_Dataset/\"+filenames[i]\n",
        "      list_of_files.append(fi)\n",
        "    \n",
        "    return list_of_files"
      ],
      "metadata": {
        "id": "9YvT7YP0xkSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preprocessing Functions"
      ],
      "metadata": {
        "id": "cn9gfxflxrPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(data):\n",
        "\n",
        "    # Parameters: data: type(string)\n",
        "    # returns: lowercase of data   \n",
        "     \n",
        "    return data.lower()"
      ],
      "metadata": {
        "id": "5nC5p6-DxkQk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def perform_word_tokenize(corpus):\n",
        "  \n",
        "    # Parameters:corpus: type(string)   \n",
        "    # returns word-level tokenization of corpus\n",
        "\n",
        "    return word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "DRiBql_LxkL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords_from_tokens(tokens, stopwords_set):\n",
        "  \n",
        "    # Parameters: tokens: type(list)\n",
        "    #             stopwords_set: type(set)\n",
        "    # returns: tokens without stopwords\n",
        "\n",
        "    tokens_sans_stopwords = [x for x in tokens if x not in stopwords_set] \n",
        "    return tokens_sans_stopwords"
      ],
      "metadata": {
        "id": "WNEgOPT-xzTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation_from_tokens(tokens):\n",
        "\n",
        "    # Parameters: tokens: type(list)\n",
        "    # returns: tokens without punctuation\n",
        "\n",
        "    tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in tokens]\n",
        "    return tokens_sans_punctuation"
      ],
      "metadata": {
        "id": "2slNeuK_xzPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_blank_space_tokens(tokens):\n",
        "    \n",
        "    #Parameters: tokens: type(list)\n",
        "    #returns: tokens without blank tokens\n",
        "\n",
        "    tokens_sans_blank_space = [x for x in tokens if x!='']  \n",
        "    return tokens_sans_blank_space"
      ],
      "metadata": {
        "id": "eww-0FRzxzN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, stopwords_set, preprocess_type):\n",
        "    # Convert the text to lower case\n",
        "    lowercase_corpus = lowercase(corpus)\n",
        "    \n",
        "    # Perform word tokenization (word_tokenize also takes care of whitespace)\n",
        "    word_tokens = perform_word_tokenize(lowercase_corpus)\n",
        "    \n",
        "    # Remove stopwords from tokens\n",
        "    word_tokens_sans_stopwords = remove_stopwords_from_tokens(word_tokens, stopwords_set)\n",
        "    \n",
        "    # Remove punctuation marks from tokens\n",
        "    word_tokens_sans_punctuation = remove_punctuation_from_tokens(word_tokens_sans_stopwords)\n",
        "    \n",
        "    # Remove blank space tokens\n",
        "    word_tokens_sans_blank_tokens = remove_blank_space_tokens(word_tokens_sans_punctuation)\n",
        "  \n",
        "    \n",
        "    if preprocess_type=='query':\n",
        "        return word_tokens_sans_blank_tokens                              # if its a query return tokens in the same order\n",
        "    return sorted(list(dict.fromkeys(word_tokens_sans_blank_tokens)))     # if its a whole document then return tokens in sorted manner"
      ],
      "metadata": {
        "id": "i1-N23x8xzJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file_dictionary(list_of_files):\n",
        "    \n",
        "    # Paramteres: list_of_files: type(string)\n",
        "    # returns: file_dictionary with integer key and path_of_file as value\n",
        "  \n",
        "    file_dictionary = {}\n",
        "    for i in range(len(list_of_files)):\n",
        "        file_dictionary[i] = list_of_files[i]\n",
        "    \n",
        "    return file_dictionary"
      ],
      "metadata": {
        "id": "F54608saxzIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q1. Data Preprocessing"
      ],
      "metadata": {
        "id": "B92O1UIg32r7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (i) Relevant Text Extraction"
      ],
      "metadata": {
        "id": "tMm3u5Gi36oE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_root='/content/CSE508_Winter2023_Dataset/'\n",
        "corpus=PlaintextCorpusReader(corpus_root,'.*')"
      ],
      "metadata": {
        "id": "CjjuqTawiSUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing names of all files\n",
        "\n",
        "filenames=corpus.fileids()\n",
        "print(filenames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWPmNl4-ifMA",
        "outputId": "530cee99-38ec-4e21-d831-ca152d11bab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cranfield0001', 'cranfield0002', 'cranfield0003', 'cranfield0004', 'cranfield0005', 'cranfield0006', 'cranfield0007', 'cranfield0008', 'cranfield0009', 'cranfield0010', 'cranfield0011', 'cranfield0012', 'cranfield0013', 'cranfield0014', 'cranfield0015', 'cranfield0016', 'cranfield0017', 'cranfield0018', 'cranfield0019', 'cranfield0020', 'cranfield0021', 'cranfield0022', 'cranfield0023', 'cranfield0024', 'cranfield0025', 'cranfield0026', 'cranfield0027', 'cranfield0028', 'cranfield0029', 'cranfield0030', 'cranfield0031', 'cranfield0032', 'cranfield0033', 'cranfield0034', 'cranfield0035', 'cranfield0036', 'cranfield0037', 'cranfield0038', 'cranfield0039', 'cranfield0040', 'cranfield0041', 'cranfield0042', 'cranfield0043', 'cranfield0044', 'cranfield0045', 'cranfield0046', 'cranfield0047', 'cranfield0048', 'cranfield0049', 'cranfield0050', 'cranfield0051', 'cranfield0052', 'cranfield0053', 'cranfield0054', 'cranfield0055', 'cranfield0056', 'cranfield0057', 'cranfield0058', 'cranfield0059', 'cranfield0060', 'cranfield0061', 'cranfield0062', 'cranfield0063', 'cranfield0064', 'cranfield0065', 'cranfield0066', 'cranfield0067', 'cranfield0068', 'cranfield0069', 'cranfield0070', 'cranfield0071', 'cranfield0072', 'cranfield0073', 'cranfield0074', 'cranfield0075', 'cranfield0076', 'cranfield0077', 'cranfield0078', 'cranfield0079', 'cranfield0080', 'cranfield0081', 'cranfield0082', 'cranfield0083', 'cranfield0084', 'cranfield0085', 'cranfield0086', 'cranfield0087', 'cranfield0088', 'cranfield0089', 'cranfield0090', 'cranfield0091', 'cranfield0092', 'cranfield0093', 'cranfield0094', 'cranfield0095', 'cranfield0096', 'cranfield0097', 'cranfield0098', 'cranfield0099', 'cranfield0100', 'cranfield0101', 'cranfield0102', 'cranfield0103', 'cranfield0104', 'cranfield0105', 'cranfield0106', 'cranfield0107', 'cranfield0108', 'cranfield0109', 'cranfield0110', 'cranfield0111', 'cranfield0112', 'cranfield0113', 'cranfield0114', 'cranfield0115', 'cranfield0116', 'cranfield0117', 'cranfield0118', 'cranfield0119', 'cranfield0120', 'cranfield0121', 'cranfield0122', 'cranfield0123', 'cranfield0124', 'cranfield0125', 'cranfield0126', 'cranfield0127', 'cranfield0128', 'cranfield0129', 'cranfield0130', 'cranfield0131', 'cranfield0132', 'cranfield0133', 'cranfield0134', 'cranfield0135', 'cranfield0136', 'cranfield0137', 'cranfield0138', 'cranfield0139', 'cranfield0140', 'cranfield0141', 'cranfield0142', 'cranfield0143', 'cranfield0144', 'cranfield0145', 'cranfield0146', 'cranfield0147', 'cranfield0148', 'cranfield0149', 'cranfield0150', 'cranfield0151', 'cranfield0152', 'cranfield0153', 'cranfield0154', 'cranfield0155', 'cranfield0156', 'cranfield0157', 'cranfield0158', 'cranfield0159', 'cranfield0160', 'cranfield0161', 'cranfield0162', 'cranfield0163', 'cranfield0164', 'cranfield0165', 'cranfield0166', 'cranfield0167', 'cranfield0168', 'cranfield0169', 'cranfield0170', 'cranfield0171', 'cranfield0172', 'cranfield0173', 'cranfield0174', 'cranfield0175', 'cranfield0176', 'cranfield0177', 'cranfield0178', 'cranfield0179', 'cranfield0180', 'cranfield0181', 'cranfield0182', 'cranfield0183', 'cranfield0184', 'cranfield0185', 'cranfield0186', 'cranfield0187', 'cranfield0188', 'cranfield0189', 'cranfield0190', 'cranfield0191', 'cranfield0192', 'cranfield0193', 'cranfield0194', 'cranfield0195', 'cranfield0196', 'cranfield0197', 'cranfield0198', 'cranfield0199', 'cranfield0200', 'cranfield0201', 'cranfield0202', 'cranfield0203', 'cranfield0204', 'cranfield0205', 'cranfield0206', 'cranfield0207', 'cranfield0208', 'cranfield0209', 'cranfield0210', 'cranfield0211', 'cranfield0212', 'cranfield0213', 'cranfield0214', 'cranfield0215', 'cranfield0216', 'cranfield0217', 'cranfield0218', 'cranfield0219', 'cranfield0220', 'cranfield0221', 'cranfield0222', 'cranfield0223', 'cranfield0224', 'cranfield0225', 'cranfield0226', 'cranfield0227', 'cranfield0228', 'cranfield0229', 'cranfield0230', 'cranfield0231', 'cranfield0232', 'cranfield0233', 'cranfield0234', 'cranfield0235', 'cranfield0236', 'cranfield0237', 'cranfield0238', 'cranfield0239', 'cranfield0240', 'cranfield0241', 'cranfield0242', 'cranfield0243', 'cranfield0244', 'cranfield0245', 'cranfield0246', 'cranfield0247', 'cranfield0248', 'cranfield0249', 'cranfield0250', 'cranfield0251', 'cranfield0252', 'cranfield0253', 'cranfield0254', 'cranfield0255', 'cranfield0256', 'cranfield0257', 'cranfield0258', 'cranfield0259', 'cranfield0260', 'cranfield0261', 'cranfield0262', 'cranfield0263', 'cranfield0264', 'cranfield0265', 'cranfield0266', 'cranfield0267', 'cranfield0268', 'cranfield0269', 'cranfield0270', 'cranfield0271', 'cranfield0272', 'cranfield0273', 'cranfield0274', 'cranfield0275', 'cranfield0276', 'cranfield0277', 'cranfield0278', 'cranfield0279', 'cranfield0280', 'cranfield0281', 'cranfield0282', 'cranfield0283', 'cranfield0284', 'cranfield0285', 'cranfield0286', 'cranfield0287', 'cranfield0288', 'cranfield0289', 'cranfield0290', 'cranfield0291', 'cranfield0292', 'cranfield0293', 'cranfield0294', 'cranfield0295', 'cranfield0296', 'cranfield0297', 'cranfield0298', 'cranfield0299', 'cranfield0300', 'cranfield0301', 'cranfield0302', 'cranfield0303', 'cranfield0304', 'cranfield0305', 'cranfield0306', 'cranfield0307', 'cranfield0308', 'cranfield0309', 'cranfield0310', 'cranfield0311', 'cranfield0312', 'cranfield0313', 'cranfield0314', 'cranfield0315', 'cranfield0316', 'cranfield0317', 'cranfield0318', 'cranfield0319', 'cranfield0320', 'cranfield0321', 'cranfield0322', 'cranfield0323', 'cranfield0324', 'cranfield0325', 'cranfield0326', 'cranfield0327', 'cranfield0328', 'cranfield0329', 'cranfield0330', 'cranfield0331', 'cranfield0332', 'cranfield0333', 'cranfield0334', 'cranfield0335', 'cranfield0336', 'cranfield0337', 'cranfield0338', 'cranfield0339', 'cranfield0340', 'cranfield0341', 'cranfield0342', 'cranfield0343', 'cranfield0344', 'cranfield0345', 'cranfield0346', 'cranfield0347', 'cranfield0348', 'cranfield0349', 'cranfield0350', 'cranfield0351', 'cranfield0352', 'cranfield0353', 'cranfield0354', 'cranfield0355', 'cranfield0356', 'cranfield0357', 'cranfield0358', 'cranfield0359', 'cranfield0360', 'cranfield0361', 'cranfield0362', 'cranfield0363', 'cranfield0364', 'cranfield0365', 'cranfield0366', 'cranfield0367', 'cranfield0368', 'cranfield0369', 'cranfield0370', 'cranfield0371', 'cranfield0372', 'cranfield0373', 'cranfield0374', 'cranfield0375', 'cranfield0376', 'cranfield0377', 'cranfield0378', 'cranfield0379', 'cranfield0380', 'cranfield0381', 'cranfield0382', 'cranfield0383', 'cranfield0384', 'cranfield0385', 'cranfield0386', 'cranfield0387', 'cranfield0388', 'cranfield0389', 'cranfield0390', 'cranfield0391', 'cranfield0392', 'cranfield0393', 'cranfield0394', 'cranfield0395', 'cranfield0396', 'cranfield0397', 'cranfield0398', 'cranfield0399', 'cranfield0400', 'cranfield0401', 'cranfield0402', 'cranfield0403', 'cranfield0404', 'cranfield0405', 'cranfield0406', 'cranfield0407', 'cranfield0408', 'cranfield0409', 'cranfield0410', 'cranfield0411', 'cranfield0412', 'cranfield0413', 'cranfield0414', 'cranfield0415', 'cranfield0416', 'cranfield0417', 'cranfield0418', 'cranfield0419', 'cranfield0420', 'cranfield0421', 'cranfield0422', 'cranfield0423', 'cranfield0424', 'cranfield0425', 'cranfield0426', 'cranfield0427', 'cranfield0428', 'cranfield0429', 'cranfield0430', 'cranfield0431', 'cranfield0432', 'cranfield0433', 'cranfield0434', 'cranfield0435', 'cranfield0436', 'cranfield0437', 'cranfield0438', 'cranfield0439', 'cranfield0440', 'cranfield0441', 'cranfield0442', 'cranfield0443', 'cranfield0444', 'cranfield0445', 'cranfield0446', 'cranfield0447', 'cranfield0448', 'cranfield0449', 'cranfield0450', 'cranfield0451', 'cranfield0452', 'cranfield0453', 'cranfield0454', 'cranfield0455', 'cranfield0456', 'cranfield0457', 'cranfield0458', 'cranfield0459', 'cranfield0460', 'cranfield0461', 'cranfield0462', 'cranfield0463', 'cranfield0464', 'cranfield0465', 'cranfield0466', 'cranfield0467', 'cranfield0468', 'cranfield0469', 'cranfield0470', 'cranfield0471', 'cranfield0472', 'cranfield0473', 'cranfield0474', 'cranfield0475', 'cranfield0476', 'cranfield0477', 'cranfield0478', 'cranfield0479', 'cranfield0480', 'cranfield0481', 'cranfield0482', 'cranfield0483', 'cranfield0484', 'cranfield0485', 'cranfield0486', 'cranfield0487', 'cranfield0488', 'cranfield0489', 'cranfield0490', 'cranfield0491', 'cranfield0492', 'cranfield0493', 'cranfield0494', 'cranfield0495', 'cranfield0496', 'cranfield0497', 'cranfield0498', 'cranfield0499', 'cranfield0500', 'cranfield0501', 'cranfield0502', 'cranfield0503', 'cranfield0504', 'cranfield0505', 'cranfield0506', 'cranfield0507', 'cranfield0508', 'cranfield0509', 'cranfield0510', 'cranfield0511', 'cranfield0512', 'cranfield0513', 'cranfield0514', 'cranfield0515', 'cranfield0516', 'cranfield0517', 'cranfield0518', 'cranfield0519', 'cranfield0520', 'cranfield0521', 'cranfield0522', 'cranfield0523', 'cranfield0524', 'cranfield0525', 'cranfield0526', 'cranfield0527', 'cranfield0528', 'cranfield0529', 'cranfield0530', 'cranfield0531', 'cranfield0532', 'cranfield0533', 'cranfield0534', 'cranfield0535', 'cranfield0536', 'cranfield0537', 'cranfield0538', 'cranfield0539', 'cranfield0540', 'cranfield0541', 'cranfield0542', 'cranfield0543', 'cranfield0544', 'cranfield0545', 'cranfield0546', 'cranfield0547', 'cranfield0548', 'cranfield0549', 'cranfield0550', 'cranfield0551', 'cranfield0552', 'cranfield0553', 'cranfield0554', 'cranfield0555', 'cranfield0556', 'cranfield0557', 'cranfield0558', 'cranfield0559', 'cranfield0560', 'cranfield0561', 'cranfield0562', 'cranfield0563', 'cranfield0564', 'cranfield0565', 'cranfield0566', 'cranfield0567', 'cranfield0568', 'cranfield0569', 'cranfield0570', 'cranfield0571', 'cranfield0572', 'cranfield0573', 'cranfield0574', 'cranfield0575', 'cranfield0576', 'cranfield0577', 'cranfield0578', 'cranfield0579', 'cranfield0580', 'cranfield0581', 'cranfield0582', 'cranfield0583', 'cranfield0584', 'cranfield0585', 'cranfield0586', 'cranfield0587', 'cranfield0588', 'cranfield0589', 'cranfield0590', 'cranfield0591', 'cranfield0592', 'cranfield0593', 'cranfield0594', 'cranfield0595', 'cranfield0596', 'cranfield0597', 'cranfield0598', 'cranfield0599', 'cranfield0600', 'cranfield0601', 'cranfield0602', 'cranfield0603', 'cranfield0604', 'cranfield0605', 'cranfield0606', 'cranfield0607', 'cranfield0608', 'cranfield0609', 'cranfield0610', 'cranfield0611', 'cranfield0612', 'cranfield0613', 'cranfield0614', 'cranfield0615', 'cranfield0616', 'cranfield0617', 'cranfield0618', 'cranfield0619', 'cranfield0620', 'cranfield0621', 'cranfield0622', 'cranfield0623', 'cranfield0624', 'cranfield0625', 'cranfield0626', 'cranfield0627', 'cranfield0628', 'cranfield0629', 'cranfield0630', 'cranfield0631', 'cranfield0632', 'cranfield0633', 'cranfield0634', 'cranfield0635', 'cranfield0636', 'cranfield0637', 'cranfield0638', 'cranfield0639', 'cranfield0640', 'cranfield0641', 'cranfield0642', 'cranfield0643', 'cranfield0644', 'cranfield0645', 'cranfield0646', 'cranfield0647', 'cranfield0648', 'cranfield0649', 'cranfield0650', 'cranfield0651', 'cranfield0652', 'cranfield0653', 'cranfield0654', 'cranfield0655', 'cranfield0656', 'cranfield0657', 'cranfield0658', 'cranfield0659', 'cranfield0660', 'cranfield0661', 'cranfield0662', 'cranfield0663', 'cranfield0664', 'cranfield0665', 'cranfield0666', 'cranfield0667', 'cranfield0668', 'cranfield0669', 'cranfield0670', 'cranfield0671', 'cranfield0672', 'cranfield0673', 'cranfield0674', 'cranfield0675', 'cranfield0676', 'cranfield0677', 'cranfield0678', 'cranfield0679', 'cranfield0680', 'cranfield0681', 'cranfield0682', 'cranfield0683', 'cranfield0684', 'cranfield0685', 'cranfield0686', 'cranfield0687', 'cranfield0688', 'cranfield0689', 'cranfield0690', 'cranfield0691', 'cranfield0692', 'cranfield0693', 'cranfield0694', 'cranfield0695', 'cranfield0696', 'cranfield0697', 'cranfield0698', 'cranfield0699', 'cranfield0700', 'cranfield0701', 'cranfield0702', 'cranfield0703', 'cranfield0704', 'cranfield0705', 'cranfield0706', 'cranfield0707', 'cranfield0708', 'cranfield0709', 'cranfield0710', 'cranfield0711', 'cranfield0712', 'cranfield0713', 'cranfield0714', 'cranfield0715', 'cranfield0716', 'cranfield0717', 'cranfield0718', 'cranfield0719', 'cranfield0720', 'cranfield0721', 'cranfield0722', 'cranfield0723', 'cranfield0724', 'cranfield0725', 'cranfield0726', 'cranfield0727', 'cranfield0728', 'cranfield0729', 'cranfield0730', 'cranfield0731', 'cranfield0732', 'cranfield0733', 'cranfield0734', 'cranfield0735', 'cranfield0736', 'cranfield0737', 'cranfield0738', 'cranfield0739', 'cranfield0740', 'cranfield0741', 'cranfield0742', 'cranfield0743', 'cranfield0744', 'cranfield0745', 'cranfield0746', 'cranfield0747', 'cranfield0748', 'cranfield0749', 'cranfield0750', 'cranfield0751', 'cranfield0752', 'cranfield0753', 'cranfield0754', 'cranfield0755', 'cranfield0756', 'cranfield0757', 'cranfield0758', 'cranfield0759', 'cranfield0760', 'cranfield0761', 'cranfield0762', 'cranfield0763', 'cranfield0764', 'cranfield0765', 'cranfield0766', 'cranfield0767', 'cranfield0768', 'cranfield0769', 'cranfield0770', 'cranfield0771', 'cranfield0772', 'cranfield0773', 'cranfield0774', 'cranfield0775', 'cranfield0776', 'cranfield0777', 'cranfield0778', 'cranfield0779', 'cranfield0780', 'cranfield0781', 'cranfield0782', 'cranfield0783', 'cranfield0784', 'cranfield0785', 'cranfield0786', 'cranfield0787', 'cranfield0788', 'cranfield0789', 'cranfield0790', 'cranfield0791', 'cranfield0792', 'cranfield0793', 'cranfield0794', 'cranfield0795', 'cranfield0796', 'cranfield0797', 'cranfield0798', 'cranfield0799', 'cranfield0800', 'cranfield0801', 'cranfield0802', 'cranfield0803', 'cranfield0804', 'cranfield0805', 'cranfield0806', 'cranfield0807', 'cranfield0808', 'cranfield0809', 'cranfield0810', 'cranfield0811', 'cranfield0812', 'cranfield0813', 'cranfield0814', 'cranfield0815', 'cranfield0816', 'cranfield0817', 'cranfield0818', 'cranfield0819', 'cranfield0820', 'cranfield0821', 'cranfield0822', 'cranfield0823', 'cranfield0824', 'cranfield0825', 'cranfield0826', 'cranfield0827', 'cranfield0828', 'cranfield0829', 'cranfield0830', 'cranfield0831', 'cranfield0832', 'cranfield0833', 'cranfield0834', 'cranfield0835', 'cranfield0836', 'cranfield0837', 'cranfield0838', 'cranfield0839', 'cranfield0840', 'cranfield0841', 'cranfield0842', 'cranfield0843', 'cranfield0844', 'cranfield0845', 'cranfield0846', 'cranfield0847', 'cranfield0848', 'cranfield0849', 'cranfield0850', 'cranfield0851', 'cranfield0852', 'cranfield0853', 'cranfield0854', 'cranfield0855', 'cranfield0856', 'cranfield0857', 'cranfield0858', 'cranfield0859', 'cranfield0860', 'cranfield0861', 'cranfield0862', 'cranfield0863', 'cranfield0864', 'cranfield0865', 'cranfield0866', 'cranfield0867', 'cranfield0868', 'cranfield0869', 'cranfield0870', 'cranfield0871', 'cranfield0872', 'cranfield0873', 'cranfield0874', 'cranfield0875', 'cranfield0876', 'cranfield0877', 'cranfield0878', 'cranfield0879', 'cranfield0880', 'cranfield0881', 'cranfield0882', 'cranfield0883', 'cranfield0884', 'cranfield0885', 'cranfield0886', 'cranfield0887', 'cranfield0888', 'cranfield0889', 'cranfield0890', 'cranfield0891', 'cranfield0892', 'cranfield0893', 'cranfield0894', 'cranfield0895', 'cranfield0896', 'cranfield0897', 'cranfield0898', 'cranfield0899', 'cranfield0900', 'cranfield0901', 'cranfield0902', 'cranfield0903', 'cranfield0904', 'cranfield0905', 'cranfield0906', 'cranfield0907', 'cranfield0908', 'cranfield0909', 'cranfield0910', 'cranfield0911', 'cranfield0912', 'cranfield0913', 'cranfield0914', 'cranfield0915', 'cranfield0916', 'cranfield0917', 'cranfield0918', 'cranfield0919', 'cranfield0920', 'cranfield0921', 'cranfield0922', 'cranfield0923', 'cranfield0924', 'cranfield0925', 'cranfield0926', 'cranfield0927', 'cranfield0928', 'cranfield0929', 'cranfield0930', 'cranfield0931', 'cranfield0932', 'cranfield0933', 'cranfield0934', 'cranfield0935', 'cranfield0936', 'cranfield0937', 'cranfield0938', 'cranfield0939', 'cranfield0940', 'cranfield0941', 'cranfield0942', 'cranfield0943', 'cranfield0944', 'cranfield0945', 'cranfield0946', 'cranfield0947', 'cranfield0948', 'cranfield0949', 'cranfield0950', 'cranfield0951', 'cranfield0952', 'cranfield0953', 'cranfield0954', 'cranfield0955', 'cranfield0956', 'cranfield0957', 'cranfield0958', 'cranfield0959', 'cranfield0960', 'cranfield0961', 'cranfield0962', 'cranfield0963', 'cranfield0964', 'cranfield0965', 'cranfield0966', 'cranfield0967', 'cranfield0968', 'cranfield0969', 'cranfield0970', 'cranfield0971', 'cranfield0972', 'cranfield0973', 'cranfield0974', 'cranfield0975', 'cranfield0976', 'cranfield0977', 'cranfield0978', 'cranfield0979', 'cranfield0980', 'cranfield0981', 'cranfield0982', 'cranfield0983', 'cranfield0984', 'cranfield0985', 'cranfield0986', 'cranfield0987', 'cranfield0988', 'cranfield0989', 'cranfield0990', 'cranfield0991', 'cranfield0992', 'cranfield0993', 'cranfield0994', 'cranfield0995', 'cranfield0996', 'cranfield0997', 'cranfield0998', 'cranfield0999', 'cranfield1000', 'cranfield1001', 'cranfield1002', 'cranfield1003', 'cranfield1004', 'cranfield1005', 'cranfield1006', 'cranfield1007', 'cranfield1008', 'cranfield1009', 'cranfield1010', 'cranfield1011', 'cranfield1012', 'cranfield1013', 'cranfield1014', 'cranfield1015', 'cranfield1016', 'cranfield1017', 'cranfield1018', 'cranfield1019', 'cranfield1020', 'cranfield1021', 'cranfield1022', 'cranfield1023', 'cranfield1024', 'cranfield1025', 'cranfield1026', 'cranfield1027', 'cranfield1028', 'cranfield1029', 'cranfield1030', 'cranfield1031', 'cranfield1032', 'cranfield1033', 'cranfield1034', 'cranfield1035', 'cranfield1036', 'cranfield1037', 'cranfield1038', 'cranfield1039', 'cranfield1040', 'cranfield1041', 'cranfield1042', 'cranfield1043', 'cranfield1044', 'cranfield1045', 'cranfield1046', 'cranfield1047', 'cranfield1048', 'cranfield1049', 'cranfield1050', 'cranfield1051', 'cranfield1052', 'cranfield1053', 'cranfield1054', 'cranfield1055', 'cranfield1056', 'cranfield1057', 'cranfield1058', 'cranfield1059', 'cranfield1060', 'cranfield1061', 'cranfield1062', 'cranfield1063', 'cranfield1064', 'cranfield1065', 'cranfield1066', 'cranfield1067', 'cranfield1068', 'cranfield1069', 'cranfield1070', 'cranfield1071', 'cranfield1072', 'cranfield1073', 'cranfield1074', 'cranfield1075', 'cranfield1076', 'cranfield1077', 'cranfield1078', 'cranfield1079', 'cranfield1080', 'cranfield1081', 'cranfield1082', 'cranfield1083', 'cranfield1084', 'cranfield1085', 'cranfield1086', 'cranfield1087', 'cranfield1088', 'cranfield1089', 'cranfield1090', 'cranfield1091', 'cranfield1092', 'cranfield1093', 'cranfield1094', 'cranfield1095', 'cranfield1096', 'cranfield1097', 'cranfield1098', 'cranfield1099', 'cranfield1100', 'cranfield1101', 'cranfield1102', 'cranfield1103', 'cranfield1104', 'cranfield1105', 'cranfield1106', 'cranfield1107', 'cranfield1108', 'cranfield1109', 'cranfield1110', 'cranfield1111', 'cranfield1112', 'cranfield1113', 'cranfield1114', 'cranfield1115', 'cranfield1116', 'cranfield1117', 'cranfield1118', 'cranfield1119', 'cranfield1120', 'cranfield1121', 'cranfield1122', 'cranfield1123', 'cranfield1124', 'cranfield1125', 'cranfield1126', 'cranfield1127', 'cranfield1128', 'cranfield1129', 'cranfield1130', 'cranfield1131', 'cranfield1132', 'cranfield1133', 'cranfield1134', 'cranfield1135', 'cranfield1136', 'cranfield1137', 'cranfield1138', 'cranfield1139', 'cranfield1140', 'cranfield1141', 'cranfield1142', 'cranfield1143', 'cranfield1144', 'cranfield1145', 'cranfield1146', 'cranfield1147', 'cranfield1148', 'cranfield1149', 'cranfield1150', 'cranfield1151', 'cranfield1152', 'cranfield1153', 'cranfield1154', 'cranfield1155', 'cranfield1156', 'cranfield1157', 'cranfield1158', 'cranfield1159', 'cranfield1160', 'cranfield1161', 'cranfield1162', 'cranfield1163', 'cranfield1164', 'cranfield1165', 'cranfield1166', 'cranfield1167', 'cranfield1168', 'cranfield1169', 'cranfield1170', 'cranfield1171', 'cranfield1172', 'cranfield1173', 'cranfield1174', 'cranfield1175', 'cranfield1176', 'cranfield1177', 'cranfield1178', 'cranfield1179', 'cranfield1180', 'cranfield1181', 'cranfield1182', 'cranfield1183', 'cranfield1184', 'cranfield1185', 'cranfield1186', 'cranfield1187', 'cranfield1188', 'cranfield1189', 'cranfield1190', 'cranfield1191', 'cranfield1192', 'cranfield1193', 'cranfield1194', 'cranfield1195', 'cranfield1196', 'cranfield1197', 'cranfield1198', 'cranfield1199', 'cranfield1200', 'cranfield1201', 'cranfield1202', 'cranfield1203', 'cranfield1204', 'cranfield1205', 'cranfield1206', 'cranfield1207', 'cranfield1208', 'cranfield1209', 'cranfield1210', 'cranfield1211', 'cranfield1212', 'cranfield1213', 'cranfield1214', 'cranfield1215', 'cranfield1216', 'cranfield1217', 'cranfield1218', 'cranfield1219', 'cranfield1220', 'cranfield1221', 'cranfield1222', 'cranfield1223', 'cranfield1224', 'cranfield1225', 'cranfield1226', 'cranfield1227', 'cranfield1228', 'cranfield1229', 'cranfield1230', 'cranfield1231', 'cranfield1232', 'cranfield1233', 'cranfield1234', 'cranfield1235', 'cranfield1236', 'cranfield1237', 'cranfield1238', 'cranfield1239', 'cranfield1240', 'cranfield1241', 'cranfield1242', 'cranfield1243', 'cranfield1244', 'cranfield1245', 'cranfield1246', 'cranfield1247', 'cranfield1248', 'cranfield1249', 'cranfield1250', 'cranfield1251', 'cranfield1252', 'cranfield1253', 'cranfield1254', 'cranfield1255', 'cranfield1256', 'cranfield1257', 'cranfield1258', 'cranfield1259', 'cranfield1260', 'cranfield1261', 'cranfield1262', 'cranfield1263', 'cranfield1264', 'cranfield1265', 'cranfield1266', 'cranfield1267', 'cranfield1268', 'cranfield1269', 'cranfield1270', 'cranfield1271', 'cranfield1272', 'cranfield1273', 'cranfield1274', 'cranfield1275', 'cranfield1276', 'cranfield1277', 'cranfield1278', 'cranfield1279', 'cranfield1280', 'cranfield1281', 'cranfield1282', 'cranfield1283', 'cranfield1284', 'cranfield1285', 'cranfield1286', 'cranfield1287', 'cranfield1288', 'cranfield1289', 'cranfield1290', 'cranfield1291', 'cranfield1292', 'cranfield1293', 'cranfield1294', 'cranfield1295', 'cranfield1296', 'cranfield1297', 'cranfield1298', 'cranfield1299', 'cranfield1300', 'cranfield1301', 'cranfield1302', 'cranfield1303', 'cranfield1304', 'cranfield1305', 'cranfield1306', 'cranfield1307', 'cranfield1308', 'cranfield1309', 'cranfield1310', 'cranfield1311', 'cranfield1312', 'cranfield1313', 'cranfield1314', 'cranfield1315', 'cranfield1316', 'cranfield1317', 'cranfield1318', 'cranfield1319', 'cranfield1320', 'cranfield1321', 'cranfield1322', 'cranfield1323', 'cranfield1324', 'cranfield1325', 'cranfield1326', 'cranfield1327', 'cranfield1328', 'cranfield1329', 'cranfield1330', 'cranfield1331', 'cranfield1332', 'cranfield1333', 'cranfield1334', 'cranfield1335', 'cranfield1336', 'cranfield1337', 'cranfield1338', 'cranfield1339', 'cranfield1340', 'cranfield1341', 'cranfield1342', 'cranfield1343', 'cranfield1344', 'cranfield1345', 'cranfield1346', 'cranfield1347', 'cranfield1348', 'cranfield1349', 'cranfield1350', 'cranfield1351', 'cranfield1352', 'cranfield1353', 'cranfield1354', 'cranfield1355', 'cranfield1356', 'cranfield1357', 'cranfield1358', 'cranfield1359', 'cranfield1360', 'cranfield1361', 'cranfield1362', 'cranfield1363', 'cranfield1364', 'cranfield1365', 'cranfield1366', 'cranfield1367', 'cranfield1368', 'cranfield1369', 'cranfield1370', 'cranfield1371', 'cranfield1372', 'cranfield1373', 'cranfield1374', 'cranfield1375', 'cranfield1376', 'cranfield1377', 'cranfield1378', 'cranfield1379', 'cranfield1380', 'cranfield1381', 'cranfield1382', 'cranfield1383', 'cranfield1384', 'cranfield1385', 'cranfield1386', 'cranfield1387', 'cranfield1388', 'cranfield1389', 'cranfield1390', 'cranfield1391', 'cranfield1392', 'cranfield1393', 'cranfield1394', 'cranfield1395', 'cranfield1396', 'cranfield1397', 'cranfield1398', 'cranfield1399', 'cranfield1400']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(filenames))\n",
        "print(len(filenames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JfQ6Yu20ikMT",
        "outputId": "f647dfb0-9cd8-4e0a-c463-b1620dbe02a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First 5 Documents"
      ],
      "metadata": {
        "id": "Y_nW0cj02aaM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing contents of first 5 files before extracting\n",
        "\n",
        "for i in range(0, 5):\n",
        "  outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  doc = outfile.read()\n",
        "  print(\"------------\")\n",
        "  print(\"Document: \"+str(i+1))\n",
        "  print(\"------------\")\n",
        "  print(doc)\n",
        "  print('-----------------------------------------------------------------')\n",
        "  print('')\n",
        "  outfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UsRuAhbiwlI",
        "outputId": "0ca5d0eb-3a73-48f0-8c08-d46a24158326"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "Document: 1\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "1\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "brenckman,m.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 25, 1958, 324.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 2\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "2\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "ting-yili\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "department of aeronautical engineering, rensselaer polytechnic\n",
            "institute\n",
            "troy, n.y.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 3\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "3\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "m. b. glauert\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "department of mathematics, university of manchester, manchester,\n",
            "england\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 4\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "4\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "yen,k.t.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 22, 1955, 728.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 5\n",
            "------------\n",
            "<DOC>\n",
            "<DOCNO>\n",
            "5\n",
            "</DOCNO>\n",
            "<TITLE>\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            "</TITLE>\n",
            "<AUTHOR>\n",
            "wasserman,b.\n",
            "</AUTHOR>\n",
            "<BIBLIO>\n",
            "j. ae. scs. 24, 1957, 924.\n",
            "</BIBLIO>\n",
            "<TEXT>\n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "</TEXT>\n",
            "</DOC>\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extracting content between TITLE and TEXT tag from all documents."
      ],
      "metadata": {
        "id": "B004slWy2oLe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(filenames)):\n",
        "    outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "    doc = outfile.read()\n",
        "\n",
        "    # initializing string\n",
        "    test_str = doc\n",
        "\n",
        "    # initializing tags\n",
        "    tag1 = \"TITLE\"\n",
        "    tag2 = \"TEXT\"\n",
        "\n",
        "    # regex to extract required strings\n",
        "    reg_str1 = \"<\"+tag1+\">(.*?)</\"+tag1+\">\"\n",
        "    res1 = re.findall(reg_str1, test_str, re.DOTALL)\n",
        "\n",
        "    reg_str2 = \"<\"+tag2+\">(.*?)</\"+tag2+\">\"\n",
        "    res2 = re.findall(reg_str2, test_str, re.DOTALL)\n",
        "\n",
        "    #Combining contents of TITLE and TEXT\n",
        "    res = res1+res2\n",
        "    \n",
        "    s = res\n",
        "\n",
        "    # using list comprehension\n",
        "    listToStr = ' '.join([str(elem) for elem in s])\n",
        "\n",
        "    writeFile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'w')\n",
        "    L = listToStr\n",
        "\n",
        "    writeFile.write(L)\n",
        "    writeFile.close()\n",
        "\n",
        "    outfile.close()"
      ],
      "metadata": {
        "id": "zyTW1S39mbe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First 5 Documents after extracting contents between TITLE and TEXT tag."
      ],
      "metadata": {
        "id": "hwG6yHOS26Pk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 5):\n",
        "  outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  doc = outfile.read()\n",
        "  print(\"------------\")\n",
        "  print(\"Document: \"+str(i+1))\n",
        "  print(\"------------\")\n",
        "  print(doc)\n",
        "  print('-----------------------------------------------------------------')\n",
        "  print('')\n",
        "  outfile.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDU8pbuGs0Fq",
        "outputId": "b7875345-dacc-447e-f24a-59a76a45bcba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------\n",
            "Document: 1\n",
            "------------\n",
            "\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            " \n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 2\n",
            "------------\n",
            "\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            " \n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 3\n",
            "------------\n",
            "\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            " \n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 4\n",
            "------------\n",
            "\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            " \n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n",
            "------------\n",
            "Document: 5\n",
            "------------\n",
            "\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            " \n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "\n",
            "-----------------------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (ii) Preprocessing"
      ],
      "metadata": {
        "id": "hR_mu5eB3R9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Lowercase the text"
      ],
      "metadata": {
        "id": "Wb8YvKiy3a4A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newfiles=[]\n",
        "for i in range(len(filenames)):\n",
        "  fileind=open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  filedata=fileind.read()\n",
        "  filedata=lowercase(filedata)\n",
        "  newfiles.append(filedata)\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uj39fCHGs0De",
        "outputId": "acd7298e-9656-42f6-f756-ee1f792f204c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "\n",
            "experimental investigation of the aerodynamics of a\n",
            "wing in a slipstream .\n",
            " \n",
            "  an experimental study of a wing in a propeller slipstream was\n",
            "made in order to determine the spanwise distribution of the lift\n",
            "increase due to slipstream at different angles of attack of the wing\n",
            "and at different free stream to slipstream velocity ratios .  the\n",
            "results were intended in part as an evaluation basis for different\n",
            "theoretical treatments of this problem .\n",
            "  the comparative span loading curves, together with supporting\n",
            "evidence, showed that a substantial part of the lift increment\n",
            "produced by the slipstream was due to a /destalling/ or boundary-layer-control\n",
            "effect .  the integrated remaining lift increment,\n",
            "after subtracting this destalling lift, was found to agree\n",
            "well with a potential flow theory .\n",
            "  an empirical evaluation of the destalling effects was made for\n",
            "the specific configuration of the experiment .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "\n",
            "simple shear flow past a flat plate in an incompressible fluid of small\n",
            "viscosity .\n",
            " \n",
            "in the study of high-speed viscous flow past a two-dimensional body it\n",
            "is usually necessary to consider a curved shock wave emitting from the\n",
            "nose or leading edge of the body .  consequently, there exists an inviscid\n",
            "rotational flow region between the shock wave and the boundary layer\n",
            ".  such a situation arises, for instance, in the study of the hypersonic\n",
            "viscous flow past a flat plate .  the situation is somewhat different\n",
            "from prandtl's classical boundary-layer problem . in prandtl's\n",
            "original problem the inviscid free stream outside the boundary layer is\n",
            "irrotational while in a hypersonic boundary-layer problem the inviscid\n",
            "free stream must be considered as rotational .  the possible effects of\n",
            "vorticity have been recently discussed by ferri and libby .  in the present\n",
            "paper, the simple shear flow past a flat plate in a fluid of small\n",
            "viscosity is investigated .  it can be shown that this problem can again\n",
            "be treated by the boundary-layer approximation, the only novel feature\n",
            "being that the free stream has a constant vorticity .  the discussion\n",
            "here is restricted to two-dimensional incompressible steady flow .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "\n",
            "the boundary layer in simple shear flow past a flat plate .\n",
            " \n",
            "the boundary-layer equations are presented for steady\n",
            "incompressible flow with no pressure gradient .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "\n",
            "approximate solutions of the incompressible laminar\n",
            "boundary layer equations for a plate in shear flow .\n",
            " \n",
            "  the two-dimensional steady boundary-layer\n",
            "problem for a flat plate in a\n",
            "shear flow of incompressible fluid is considered .\n",
            "solutions for the boundarylayer\n",
            "thickness, skin friction, and the velocity\n",
            "distribution in the boundary\n",
            "layer are obtained by the karman-pohlhausen\n",
            "technique .  comparison with\n",
            "the boundary layer of a uniform flow has also\n",
            "been made to show the effect of\n",
            "vorticity .\n",
            "\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "\n",
            "one-dimensional transient heat conduction into a double-layer\n",
            "slab subjected to a linear heat input for a small time\n",
            "internal .\n",
            " \n",
            "  analytic solutions are presented for the transient heat conduction\n",
            "in composite slabs exposed at one surface to a\n",
            "triangular heat rate .  this type of heating rate may occur, for\n",
            "example, during aerodynamic heating .\n",
            "\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Perform tokenization"
      ],
      "metadata": {
        "id": "qWubxSm33gUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  filedata=perform_word_tokenize(filedata)\n",
        "  newfiles[i] = filedata\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4Bw0AHwvgtS",
        "outputId": "ebd53f3c-980a-4673-b3e6-fe2533c8d6dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'of', 'the', 'aerodynamics', 'of', 'a', 'wing', 'in', 'a', 'slipstream', '.', 'an', 'experimental', 'study', 'of', 'a', 'wing', 'in', 'a', 'propeller', 'slipstream', 'was', 'made', 'in', 'order', 'to', 'determine', 'the', 'spanwise', 'distribution', 'of', 'the', 'lift', 'increase', 'due', 'to', 'slipstream', 'at', 'different', 'angles', 'of', 'attack', 'of', 'the', 'wing', 'and', 'at', 'different', 'free', 'stream', 'to', 'slipstream', 'velocity', 'ratios', '.', 'the', 'results', 'were', 'intended', 'in', 'part', 'as', 'an', 'evaluation', 'basis', 'for', 'different', 'theoretical', 'treatments', 'of', 'this', 'problem', '.', 'the', 'comparative', 'span', 'loading', 'curves', ',', 'together', 'with', 'supporting', 'evidence', ',', 'showed', 'that', 'a', 'substantial', 'part', 'of', 'the', 'lift', 'increment', 'produced', 'by', 'the', 'slipstream', 'was', 'due', 'to', 'a', '/destalling/', 'or', 'boundary-layer-control', 'effect', '.', 'the', 'integrated', 'remaining', 'lift', 'increment', ',', 'after', 'subtracting', 'this', 'destalling', 'lift', ',', 'was', 'found', 'to', 'agree', 'well', 'with', 'a', 'potential', 'flow', 'theory', '.', 'an', 'empirical', 'evaluation', 'of', 'the', 'destalling', 'effects', 'was', 'made', 'for', 'the', 'specific', 'configuration', 'of', 'the', 'experiment', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', 'in', 'an', 'incompressible', 'fluid', 'of', 'small', 'viscosity', '.', 'in', 'the', 'study', 'of', 'high-speed', 'viscous', 'flow', 'past', 'a', 'two-dimensional', 'body', 'it', 'is', 'usually', 'necessary', 'to', 'consider', 'a', 'curved', 'shock', 'wave', 'emitting', 'from', 'the', 'nose', 'or', 'leading', 'edge', 'of', 'the', 'body', '.', 'consequently', ',', 'there', 'exists', 'an', 'inviscid', 'rotational', 'flow', 'region', 'between', 'the', 'shock', 'wave', 'and', 'the', 'boundary', 'layer', '.', 'such', 'a', 'situation', 'arises', ',', 'for', 'instance', ',', 'in', 'the', 'study', 'of', 'the', 'hypersonic', 'viscous', 'flow', 'past', 'a', 'flat', 'plate', '.', 'the', 'situation', 'is', 'somewhat', 'different', 'from', 'prandtl', \"'s\", 'classical', 'boundary-layer', 'problem', '.', 'in', \"prandtl's\", 'original', 'problem', 'the', 'inviscid', 'free', 'stream', 'outside', 'the', 'boundary', 'layer', 'is', 'irrotational', 'while', 'in', 'a', 'hypersonic', 'boundary-layer', 'problem', 'the', 'inviscid', 'free', 'stream', 'must', 'be', 'considered', 'as', 'rotational', '.', 'the', 'possible', 'effects', 'of', 'vorticity', 'have', 'been', 'recently', 'discussed', 'by', 'ferri', 'and', 'libby', '.', 'in', 'the', 'present', 'paper', ',', 'the', 'simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', 'in', 'a', 'fluid', 'of', 'small', 'viscosity', 'is', 'investigated', '.', 'it', 'can', 'be', 'shown', 'that', 'this', 'problem', 'can', 'again', 'be', 'treated', 'by', 'the', 'boundary-layer', 'approximation', ',', 'the', 'only', 'novel', 'feature', 'being', 'that', 'the', 'free', 'stream', 'has', 'a', 'constant', 'vorticity', '.', 'the', 'discussion', 'here', 'is', 'restricted', 'to', 'two-dimensional', 'incompressible', 'steady', 'flow', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['the', 'boundary', 'layer', 'in', 'simple', 'shear', 'flow', 'past', 'a', 'flat', 'plate', '.', 'the', 'boundary-layer', 'equations', 'are', 'presented', 'for', 'steady', 'incompressible', 'flow', 'with', 'no', 'pressure', 'gradient', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'of', 'the', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'for', 'a', 'plate', 'in', 'shear', 'flow', '.', 'the', 'two-dimensional', 'steady', 'boundary-layer', 'problem', 'for', 'a', 'flat', 'plate', 'in', 'a', 'shear', 'flow', 'of', 'incompressible', 'fluid', 'is', 'considered', '.', 'solutions', 'for', 'the', 'boundarylayer', 'thickness', ',', 'skin', 'friction', ',', 'and', 'the', 'velocity', 'distribution', 'in', 'the', 'boundary', 'layer', 'are', 'obtained', 'by', 'the', 'karman-pohlhausen', 'technique', '.', 'comparison', 'with', 'the', 'boundary', 'layer', 'of', 'a', 'uniform', 'flow', 'has', 'also', 'been', 'made', 'to', 'show', 'the', 'effect', 'of', 'vorticity', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['one-dimensional', 'transient', 'heat', 'conduction', 'into', 'a', 'double-layer', 'slab', 'subjected', 'to', 'a', 'linear', 'heat', 'input', 'for', 'a', 'small', 'time', 'internal', '.', 'analytic', 'solutions', 'are', 'presented', 'for', 'the', 'transient', 'heat', 'conduction', 'in', 'composite', 'slabs', 'exposed', 'at', 'one', 'surface', 'to', 'a', 'triangular', 'heat', 'rate', '.', 'this', 'type', 'of', 'heating', 'rate', 'may', 'occur', ',', 'for', 'example', ',', 'during', 'aerodynamic', 'heating', '.']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "JHqdAX-JvghM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Remove stopwords"
      ],
      "metadata": {
        "id": "vGceyDFv3k3O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_stopwords = remove_stopwords_from_tokens(filedata, stopwords_set)\n",
        "  newfiles[i] = tokens_sans_stopwords\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRJSvynOvgWx",
        "outputId": "ae595c25-f4fa-4d2a-cb0e-c023877e3925"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', '.', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', '.', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', '.', 'comparative', 'span', 'loading', 'curves', ',', 'together', 'supporting', 'evidence', ',', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', '/destalling/', 'boundary-layer-control', 'effect', '.', 'integrated', 'remaining', 'lift', 'increment', ',', 'subtracting', 'destalling', 'lift', ',', 'found', 'agree', 'well', 'potential', 'flow', 'theory', '.', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', '.', 'study', 'high-speed', 'viscous', 'flow', 'past', 'two-dimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', '.', 'consequently', ',', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', '.', 'situation', 'arises', ',', 'instance', ',', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', '.', 'situation', 'somewhat', 'different', 'prandtl', \"'s\", 'classical', 'boundary-layer', 'problem', '.', \"prandtl's\", 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundary-layer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', '.', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', '.', 'present', 'paper', ',', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', '.', 'shown', 'problem', 'treated', 'boundary-layer', 'approximation', ',', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', '.', 'discussion', 'restricted', 'two-dimensional', 'incompressible', 'steady', 'flow', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', '.', 'boundary-layer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', '.', 'two-dimensional', 'steady', 'boundary-layer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', '.', 'solutions', 'boundarylayer', 'thickness', ',', 'skin', 'friction', ',', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karman-pohlhausen', 'technique', '.', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity', '.']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['one-dimensional', 'transient', 'heat', 'conduction', 'double-layer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', '.', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', '.', 'type', 'heating', 'rate', 'may', 'occur', ',', 'example', ',', 'aerodynamic', 'heating', '.']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Remove punctuations"
      ],
      "metadata": {
        "id": "-litrUFw3ocO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_punctuation = remove_punctuation_from_tokens(filedata)\n",
        "  newfiles[i] = tokens_sans_punctuation\n",
        "print(len(newfiles))\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X00UdJZpszv0",
        "outputId": "d6e67a84-c520-4b67-f78f-47a4aa85ba5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', '', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', '', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', '', 'comparative', 'span', 'loading', 'curves', '', 'together', 'supporting', 'evidence', '', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', 'destalling', 'boundarylayercontrol', 'effect', '', 'integrated', 'remaining', 'lift', 'increment', '', 'subtracting', 'destalling', 'lift', '', 'found', 'agree', 'well', 'potential', 'flow', 'theory', '', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', '', 'study', 'highspeed', 'viscous', 'flow', 'past', 'twodimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', '', 'consequently', '', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', '', 'situation', 'arises', '', 'instance', '', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', '', 'situation', 'somewhat', 'different', 'prandtl', 's', 'classical', 'boundarylayer', 'problem', '', 'prandtls', 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundarylayer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', '', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', '', 'present', 'paper', '', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', '', 'shown', 'problem', 'treated', 'boundarylayer', 'approximation', '', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', '', 'discussion', 'restricted', 'twodimensional', 'incompressible', 'steady', 'flow', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', '', 'boundarylayer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', '', 'twodimensional', 'steady', 'boundarylayer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', '', 'solutions', 'boundarylayer', 'thickness', '', 'skin', 'friction', '', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karmanpohlhausen', 'technique', '', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity', '']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['onedimensional', 'transient', 'heat', 'conduction', 'doublelayer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', '', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', '', 'type', 'heating', 'rate', 'may', 'occur', '', 'example', '', 'aerodynamic', 'heating', '']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Remove blank space tokens"
      ],
      "metadata": {
        "id": "3FMLU0E-3spS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lent = 0\n",
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  tokens_sans_blank_space = remove_blank_space_tokens(filedata)\n",
        "  newfiles[i] = tokens_sans_blank_space\n",
        "  lent += len(newfiles[i])\n",
        "print(len(newfiles))\n",
        "print(lent)\n",
        "\n",
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwvbE8aIszs4",
        "outputId": "31970c9f-ca13-44cf-f551-da508c07c605"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n",
            "127377\n",
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental', 'investigation', 'aerodynamics', 'wing', 'slipstream', 'experimental', 'study', 'wing', 'propeller', 'slipstream', 'made', 'order', 'determine', 'spanwise', 'distribution', 'lift', 'increase', 'due', 'slipstream', 'different', 'angles', 'attack', 'wing', 'different', 'free', 'stream', 'slipstream', 'velocity', 'ratios', 'results', 'intended', 'part', 'evaluation', 'basis', 'different', 'theoretical', 'treatments', 'problem', 'comparative', 'span', 'loading', 'curves', 'together', 'supporting', 'evidence', 'showed', 'substantial', 'part', 'lift', 'increment', 'produced', 'slipstream', 'due', 'destalling', 'boundarylayercontrol', 'effect', 'integrated', 'remaining', 'lift', 'increment', 'subtracting', 'destalling', 'lift', 'found', 'agree', 'well', 'potential', 'flow', 'theory', 'empirical', 'evaluation', 'destalling', 'effects', 'made', 'specific', 'configuration', 'experiment']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple', 'shear', 'flow', 'past', 'flat', 'plate', 'incompressible', 'fluid', 'small', 'viscosity', 'study', 'highspeed', 'viscous', 'flow', 'past', 'twodimensional', 'body', 'usually', 'necessary', 'consider', 'curved', 'shock', 'wave', 'emitting', 'nose', 'leading', 'edge', 'body', 'consequently', 'exists', 'inviscid', 'rotational', 'flow', 'region', 'shock', 'wave', 'boundary', 'layer', 'situation', 'arises', 'instance', 'study', 'hypersonic', 'viscous', 'flow', 'past', 'flat', 'plate', 'situation', 'somewhat', 'different', 'prandtl', 's', 'classical', 'boundarylayer', 'problem', 'prandtls', 'original', 'problem', 'inviscid', 'free', 'stream', 'outside', 'boundary', 'layer', 'irrotational', 'hypersonic', 'boundarylayer', 'problem', 'inviscid', 'free', 'stream', 'must', 'considered', 'rotational', 'possible', 'effects', 'vorticity', 'recently', 'discussed', 'ferri', 'libby', 'present', 'paper', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'fluid', 'small', 'viscosity', 'investigated', 'shown', 'problem', 'treated', 'boundarylayer', 'approximation', 'novel', 'feature', 'free', 'stream', 'constant', 'vorticity', 'discussion', 'restricted', 'twodimensional', 'incompressible', 'steady', 'flow']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary', 'layer', 'simple', 'shear', 'flow', 'past', 'flat', 'plate', 'boundarylayer', 'equations', 'presented', 'steady', 'incompressible', 'flow', 'pressure', 'gradient']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate', 'solutions', 'incompressible', 'laminar', 'boundary', 'layer', 'equations', 'plate', 'shear', 'flow', 'twodimensional', 'steady', 'boundarylayer', 'problem', 'flat', 'plate', 'shear', 'flow', 'incompressible', 'fluid', 'considered', 'solutions', 'boundarylayer', 'thickness', 'skin', 'friction', 'velocity', 'distribution', 'boundary', 'layer', 'obtained', 'karmanpohlhausen', 'technique', 'comparison', 'boundary', 'layer', 'uniform', 'flow', 'also', 'made', 'show', 'effect', 'vorticity']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['onedimensional', 'transient', 'heat', 'conduction', 'doublelayer', 'slab', 'subjected', 'linear', 'heat', 'input', 'small', 'time', 'internal', 'analytic', 'solutions', 'presented', 'transient', 'heat', 'conduction', 'composite', 'slabs', 'exposed', 'one', 'surface', 'triangular', 'heat', 'rate', 'type', 'heating', 'rate', 'may', 'occur', 'example', 'aerodynamic', 'heating']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fileData = newfiles"
      ],
      "metadata": {
        "id": "JablNRzcszln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# QUESTION 2 STARTS HERE\n"
      ],
      "metadata": {
        "id": "5v4VqJWSCHpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Q2. Boolean Queries"
      ],
      "metadata": {
        "id": "NRru76l0DWit"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Create a unigram inverted index(from scratch; No library allowed) of the dataset obtained from Q1 (after preprocessing)."
      ],
      "metadata": {
        "id": "peVloS7fDbbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Use Pythons pickle module to save and load the unigram inverted index."
      ],
      "metadata": {
        "id": "v8oe0ZKQ4q9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_unigram_inverted_index(file_dictionary, stopwords_set):\n",
        "    # initialize unigram inverted index\n",
        "    unigram_inverted_index = {}\n",
        "    \n",
        "    # unigram inverted index\n",
        "    for doc_ID in range(len(file_dictionary)):\n",
        "        file = open(file_dictionary[doc_ID], 'r', encoding='utf-8', errors='ignore')\n",
        "        file_corpus = file.read()\n",
        "        file.close()\n",
        "        doc_tokens = preprocess(file_corpus, stopwords_set, 'doc')\n",
        "        for token in doc_tokens:\n",
        "            if token in unigram_inverted_index:\n",
        "                unigram_inverted_index[token][0] += 1               # stores doc_frequency\n",
        "                unigram_inverted_index[token][1].append(doc_ID)     # appending docs in which that term occurs\n",
        "            else:\n",
        "                unigram_inverted_index[token] = []\n",
        "                unigram_inverted_index[token].append(1)\n",
        "                unigram_inverted_index[token].append([doc_ID])      # initializing if this token appears for first time\n",
        "    \n",
        "    # Saving unigram inverted index using pickle module  \n",
        "    uii_file = open('unigram_inverted_index_pickle_file', 'wb')\n",
        "    pickle.dump(unigram_inverted_index, uii_file)\n",
        "    uii_file.close()\n",
        "\n",
        "    return unigram_inverted_index\n",
        "\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "list_of_files = getListOfFiles('/content/CSE508_Winter2023_Dataset/')\n",
        "uni = create_unigram_inverted_index(list_of_files,stopwords_set)"
      ],
      "metadata": {
        "id": "ZADY2iouszjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uii_file = open('unigram_inverted_index_pickle_file', 'rb')\n",
        "unigram_inverted_index = pickle.load(uii_file)\n",
        "uii_file.close()"
      ],
      "metadata": {
        "id": "aiGvgaKfjc5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(unigram_inverted_index))"
      ],
      "metadata": {
        "id": "H75rMcN5j88G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b053fbf-cbd8-42d9-db4b-565257463ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Provide support for the following operations: \n",
        "a. T1 AND T2<br>\n",
        "b. T1 AND NOT T2<br>\n",
        "c. T1 OR T2<br>\n",
        "d. T1 OR NOT T2"
      ],
      "metadata": {
        "id": "3Cz0lNlV42Cc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AND"
      ],
      "metadata": {
        "id": "O1oj5ZZR5esI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xANDy(pos_list_1, pos_list_2):\n",
        "    result = SortedSet()\n",
        "    i,j = 0,0\n",
        "    num_of_operations = 0\n",
        "    while(i<len(pos_list_1) and j<len(pos_list_2)):\n",
        "        if pos_list_1[i]==pos_list_2[j]:\n",
        "            result.add(pos_list_1[i])\n",
        "            i+=1\n",
        "            j+=1\n",
        "        elif pos_list_1[i]<pos_list_2[j]:\n",
        "            i+=1\n",
        "        else:\n",
        "            j+=1\n",
        "        num_of_operations+=1\n",
        "    \n",
        "    return result, num_of_operations"
      ],
      "metadata": {
        "id": "tzreAL103-QI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR"
      ],
      "metadata": {
        "id": "mI2KNVBh5kiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xORy(pos_list_1, pos_list_2):\n",
        "    result = SortedSet()\n",
        "    i,j = 0,0\n",
        "    num_of_operations = 0\n",
        "    while(i<len(pos_list_1) and j<len(pos_list_2)):\n",
        "        if pos_list_1[i]==pos_list_2[j]:\n",
        "            result.add(pos_list_1[i])\n",
        "            i+=1\n",
        "            j+=1\n",
        "        elif pos_list_1[i]<pos_list_2[j]:\n",
        "            result.add(pos_list_1[i])\n",
        "            i+=1\n",
        "        else:\n",
        "            result.add(pos_list_2[j])\n",
        "            j+=1\n",
        "        num_of_operations+=1\n",
        "    \n",
        "    while(i<len(pos_list_1)):\n",
        "        result.add(pos_list_1[i])\n",
        "        i+=1\n",
        "    while(j<len(pos_list_2)):\n",
        "        result.add(pos_list_2[j])\n",
        "        j+=1\n",
        "    \n",
        "    return result, num_of_operations"
      ],
      "metadata": {
        "id": "CwkXBpjo5gCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AND NOT"
      ],
      "metadata": {
        "id": "QScbMsi55rqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xANDNOTy(pos_list_1, pos_list_2):\n",
        "    result = SortedSet()\n",
        "    i,j = 0,0\n",
        "    num_of_operations = 0\n",
        "    while(i<len(pos_list_1) and j<len(pos_list_2)):\n",
        "        if pos_list_1[i]==pos_list_2[j]:\n",
        "            i+=1\n",
        "            j+=1\n",
        "        elif pos_list_1[i]<pos_list_2[j]:\n",
        "            result.add(pos_list_1[i])\n",
        "            i+=1\n",
        "        else:\n",
        "            j+=1\n",
        "        num_of_operations+=1\n",
        "    \n",
        "    while(i<len(pos_list_1)):\n",
        "        result.add(pos_list_1[i])\n",
        "        i+=1\n",
        "    \n",
        "    return result, num_of_operations"
      ],
      "metadata": {
        "id": "vUEbENl95ma7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "OR NOT"
      ],
      "metadata": {
        "id": "twuRTZdb53gz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def xORNOTy(pos_list_1, pos_list_2, u_list):\n",
        "    num_of_not_operations = 0\n",
        "    i,j = 0,0\n",
        "    not_pos_list_2 = SortedSet()\n",
        "    while(i<len(pos_list_2) and j<len(u_list)):\n",
        "        if pos_list_2[i]<u_list[j]:\n",
        "            i+=1\n",
        "        elif pos_list_2[i]>u_list[j]:\n",
        "            not_pos_list_2.add(u_list[j])\n",
        "            j+=1\n",
        "        else:\n",
        "            i+=1\n",
        "            j+=1\n",
        "        num_of_not_operations+=1\n",
        "    while(j<len(u_list)):\n",
        "        not_pos_list_2.add(u_list[j])\n",
        "        j+=1\n",
        "    \n",
        "    result, ops = xORy(pos_list_1, not_pos_list_2)\n",
        "    return result, num_of_not_operations+ops"
      ],
      "metadata": {
        "id": "U2DQN7Et5ynp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Queries should be generalized i.e., you should provide support for queries like T1 AND T2 OR T3 OR T4 ...."
      ],
      "metadata": {
        "id": "xd2b5rDC6eQQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. You are also required to compute the minimum number of comparisons done to execute the query [only where merging is required]"
      ],
      "metadata": {
        "id": "8sAvW61c61fO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Input format: <br>\n",
        "a. The first line contains N denoting the number of queries to execute<br>\n",
        "b. The next 2N lines contain queries in the following format:<br>\n",
        "&ensp; i. Input sequence<br>\n",
        "&ensp; ii. Operations separated by comma"
      ],
      "metadata": {
        "id": "XBAFpN1W7ZTX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Output Format:\n",
        "a. 4N lines consisting of the results in the following format:<br>\n",
        "&ensp;i. Query X<br>\n",
        "&ensp;ii. Number of documents retrieved for query X<br>\n",
        "&ensp;iii. Names of the documents retrieved for query X<br>\n",
        "&ensp;iv. Number of comparisons required for query X<br>"
      ],
      "metadata": {
        "id": "LOYk4XwI8Vli"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Perform preprocessing steps (from Q1) on the input sequence as well."
      ],
      "metadata": {
        "id": "x7KgEgio7EfS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # create set of stop words for preprocessing\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    \n",
        "    # Get List of Files in Dataset\n",
        "    list_of_files = getListOfFiles('/content/CSE508_Winter2023_Dataset/')\n",
        "    \n",
        "    # create dictionary of file with docID (integer) as key and full_path of file as value\n",
        "    file_dictionary = create_file_dictionary(list_of_files)\n",
        "    \n",
        "    # create unigram inverted index once and then load pickle file afterwards\n",
        "    create_unigram_inverted_index(file_dictionary, stopwords_set)\n",
        "    \n",
        "    #Loading pre-processed files\n",
        "    uii_file = open('unigram_inverted_index_pickle_file', 'rb')\n",
        "    unigram_inverted_index = pickle.load(uii_file)\n",
        "    uii_file.close()\n",
        "    u_list = SortedSet()\n",
        "    for a in range(len(file_dictionary)):\n",
        "        u_list.add(a)\n",
        "    \n",
        "    N = int(input())                                                            # N denoting the number of queries to execute\n",
        "    for q in range(N):\n",
        "        input_sentence = input()                                                # Taking input query\n",
        "        input_operation_sequence = input()                                      # Taking operations to be done as an input \n",
        "        sanitized_query = preprocess(input_sentence, stopwords_set, 'query')    # Preprocessing the input query\n",
        "        sanitized_operation_sequence = []                             \n",
        "        for op_seq in input_operation_sequence.split(','):\n",
        "            sanitized_operation_sequence.append(op_seq.lower().strip())         \n",
        "        print(sanitized_query)                                                  # Printing Input sequence\n",
        "        print(sanitized_operation_sequence)                                     # Printing operations seperated by comma\n",
        "        if(len(sanitized_query)!=len(sanitized_operation_sequence)+1):          \n",
        "            print('invalid query')\n",
        "            continue    # will continue with next query \n",
        "        ptr1,ptr2 = 0,0\n",
        "        result = unigram_inverted_index[sanitized_query[ptr1]][1]\n",
        "        num_of_operations = 0\n",
        "        while(ptr2<len(sanitized_operation_sequence)):                          # Processing of the input query X starts here.\n",
        "            if sanitized_operation_sequence[ptr2]=='and':\n",
        "                res, ops = xANDy(result, unigram_inverted_index[sanitized_query[ptr1+1]][1])\n",
        "            elif sanitized_operation_sequence[ptr2]=='or':\n",
        "                res, ops = xORy(result, unigram_inverted_index[sanitized_query[ptr1+1]][1])\n",
        "            elif sanitized_operation_sequence[ptr2]=='and not':\n",
        "                res, ops = xANDNOTy(result, unigram_inverted_index[sanitized_query[ptr1+1]][1])\n",
        "            elif sanitized_operation_sequence[ptr2]=='or not':\n",
        "                res, ops = xORNOTy(result, unigram_inverted_index[sanitized_query[ptr1+1]][1], u_list)\n",
        "            ptr1+=1\n",
        "            ptr2+=1\n",
        "            result = res\n",
        "            num_of_operations += ops\n",
        "        print('Number of documents matched: {}'.format(len(result)))            # Printing Number of documents retrived for query X.\n",
        "        print('Number of comparisons required: {}'.format(num_of_operations))   # Printing Number of comparisons required for query X.\n",
        "        print('Documents: \\n')                                                  # Printing Name of Documents retrived for query X.\n",
        "        for res_doc in result:\n",
        "            print(file_dictionary[res_doc])"
      ],
      "metadata": {
        "id": "rjtOa6Yf5uk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "uZ-_POQl6qbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b7763e-53de-405e-9ce1-edffbf44b3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "transition supersonic wind\n",
            "and, and\n",
            "['transition', 'supersonic', 'wind']\n",
            "['and', 'and']\n",
            "Number of documents matched: 7\n",
            "Number of comparisons required: 434\n",
            "Documents: \n",
            "\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0007\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0040\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0171\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0182\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0272\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0710\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(0, 5):\n",
        "  print('----------')\n",
        "  print('File : '+str(i+1))\n",
        "  print('----------')\n",
        "  print(newfiles[i])\n",
        "  print('--------------------------------------------------------------------')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGunUKnqVhEP",
        "outputId": "ff19f571-0e37-4c64-be21-b39abab8dd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------\n",
            "File : 1\n",
            "----------\n",
            "['experimental investigation', 'investigation aerodynamics', 'aerodynamics wing', 'wing slipstream', 'slipstream experimental', 'experimental study', 'study wing', 'wing propeller', 'propeller slipstream', 'slipstream made', 'made order', 'order determine', 'determine spanwise', 'spanwise distribution', 'distribution lift', 'lift increase', 'increase due', 'due slipstream', 'slipstream different', 'different angles', 'angles attack', 'attack wing', 'wing different', 'different free', 'free stream', 'stream slipstream', 'slipstream velocity', 'velocity ratios', 'ratios results', 'results intended', 'intended part', 'part evaluation', 'evaluation basis', 'basis different', 'different theoretical', 'theoretical treatments', 'treatments problem', 'problem comparative', 'comparative span', 'span loading', 'loading curves', 'curves together', 'together supporting', 'supporting evidence', 'evidence showed', 'showed substantial', 'substantial part', 'part lift', 'lift increment', 'increment produced', 'produced slipstream', 'slipstream due', 'due destalling', 'destalling boundarylayercontrol', 'boundarylayercontrol effect', 'effect integrated', 'integrated remaining', 'remaining lift', 'lift increment', 'increment subtracting', 'subtracting destalling', 'destalling lift', 'lift found', 'found agree', 'agree well', 'well potential', 'potential flow', 'flow theory', 'theory empirical', 'empirical evaluation', 'evaluation destalling', 'destalling effects', 'effects made', 'made specific', 'specific configuration', 'configuration experiment']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 2\n",
            "----------\n",
            "['simple shear', 'shear flow', 'flow past', 'past flat', 'flat plate', 'plate incompressible', 'incompressible fluid', 'fluid small', 'small viscosity', 'viscosity study', 'study highspeed', 'highspeed viscous', 'viscous flow', 'flow past', 'past twodimensional', 'twodimensional body', 'body usually', 'usually necessary', 'necessary consider', 'consider curved', 'curved shock', 'shock wave', 'wave emitting', 'emitting nose', 'nose leading', 'leading edge', 'edge body', 'body consequently', 'consequently exists', 'exists inviscid', 'inviscid rotational', 'rotational flow', 'flow region', 'region shock', 'shock wave', 'wave boundary', 'boundary layer', 'layer situation', 'situation arises', 'arises instance', 'instance study', 'study hypersonic', 'hypersonic viscous', 'viscous flow', 'flow past', 'past flat', 'flat plate', 'plate situation', 'situation somewhat', 'somewhat different', 'different prandtl', 'prandtl s', 's classical', 'classical boundarylayer', 'boundarylayer problem', 'problem prandtls', 'prandtls original', 'original problem', 'problem inviscid', 'inviscid free', 'free stream', 'stream outside', 'outside boundary', 'boundary layer', 'layer irrotational', 'irrotational hypersonic', 'hypersonic boundarylayer', 'boundarylayer problem', 'problem inviscid', 'inviscid free', 'free stream', 'stream must', 'must considered', 'considered rotational', 'rotational possible', 'possible effects', 'effects vorticity', 'vorticity recently', 'recently discussed', 'discussed ferri', 'ferri libby', 'libby present', 'present paper', 'paper simple', 'simple shear', 'shear flow', 'flow past', 'past flat', 'flat plate', 'plate fluid', 'fluid small', 'small viscosity', 'viscosity investigated', 'investigated shown', 'shown problem', 'problem treated', 'treated boundarylayer', 'boundarylayer approximation', 'approximation novel', 'novel feature', 'feature free', 'free stream', 'stream constant', 'constant vorticity', 'vorticity discussion', 'discussion restricted', 'restricted twodimensional', 'twodimensional incompressible', 'incompressible steady', 'steady flow']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 3\n",
            "----------\n",
            "['boundary layer', 'layer simple', 'simple shear', 'shear flow', 'flow past', 'past flat', 'flat plate', 'plate boundarylayer', 'boundarylayer equations', 'equations presented', 'presented steady', 'steady incompressible', 'incompressible flow', 'flow pressure', 'pressure gradient']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 4\n",
            "----------\n",
            "['approximate solutions', 'solutions incompressible', 'incompressible laminar', 'laminar boundary', 'boundary layer', 'layer equations', 'equations plate', 'plate shear', 'shear flow', 'flow twodimensional', 'twodimensional steady', 'steady boundarylayer', 'boundarylayer problem', 'problem flat', 'flat plate', 'plate shear', 'shear flow', 'flow incompressible', 'incompressible fluid', 'fluid considered', 'considered solutions', 'solutions boundarylayer', 'boundarylayer thickness', 'thickness skin', 'skin friction', 'friction velocity', 'velocity distribution', 'distribution boundary', 'boundary layer', 'layer obtained', 'obtained karmanpohlhausen', 'karmanpohlhausen technique', 'technique comparison', 'comparison boundary', 'boundary layer', 'layer uniform', 'uniform flow', 'flow also', 'also made', 'made show', 'show effect', 'effect vorticity']\n",
            "--------------------------------------------------------------------\n",
            "----------\n",
            "File : 5\n",
            "----------\n",
            "['onedimensional transient', 'transient heat', 'heat conduction', 'conduction doublelayer', 'doublelayer slab', 'slab subjected', 'subjected linear', 'linear heat', 'heat input', 'input small', 'small time', 'time internal', 'internal analytic', 'analytic solutions', 'solutions presented', 'presented transient', 'transient heat', 'heat conduction', 'conduction composite', 'composite slabs', 'slabs exposed', 'exposed one', 'one surface', 'surface triangular', 'triangular heat', 'heat rate', 'rate type', 'type heating', 'heating rate', 'rate may', 'may occur', 'occur example', 'example aerodynamic', 'aerodynamic heating']\n",
            "--------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "newlist=[]\n",
        "newlist=newfiles\n",
        "for i in range(len(newfiles)):\n",
        "   for j in range(len(newfiles[i])-1):\n",
        "       temp1=newfiles[i][j]\n",
        "       temp2=newfiles[i][j+1]\n",
        "       temp=temp1+' '+temp2\n",
        "       newlist[i][j]=temp\n",
        "\n",
        "   newlist[i].pop()\n",
        "bigram_inverted_index = {}\n",
        "for i in range(len(newlist)):\n",
        "        # file = open(file_dictionary[doc_ID], 'r', encoding='utf-8', errors='ignore')\n",
        "        # file_corpus = file.read()\n",
        "        # file.close()\n",
        "        # doc_tokens = preprocess(file_corpus, stopwords_set, 'doc')\n",
        "    doc_tokens = newlist[i]\n",
        "    for token in doc_tokens:\n",
        "        if token in bigram_inverted_index:\n",
        "            bigram_inverted_index[token][0] += 1   # stores doc_frequency\n",
        "            if( i not in bigram_inverted_index[token][1]):\n",
        "              bigram_inverted_index[token][1].append(i)    # appending docs in which that term occurs\n",
        "              \n",
        "        else:\n",
        "            bigram_inverted_index[token] = []\n",
        "            bigram_inverted_index[token].append(1)\n",
        "            bigram_inverted_index[token].append([i])    # initializing if this token appears for first time'\n",
        "pi_file = open('bigram_inverted_index_pickle_file', 'wb')\n",
        "pickle.dump(bigram_inverted_index, pi_file)\n",
        "pi_file.close()"
      ],
      "metadata": {
        "id": "o_XLZMddhals"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_file = open('bigram_inverted_index_pickle_file', 'rb')\n",
        "bigram_inverted_index = pickle.load(bigram_file)\n",
        "bigram_file.close()"
      ],
      "metadata": {
        "id": "idJv1Jzpm-og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uii_file = open('unigram_inverted_index_pickle_file', 'rb')\n",
        "unigram_inverted_index = pickle.load(uii_file)\n",
        "uii_file.close()"
      ],
      "metadata": {
        "id": "8frd80bhWVPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "b=search_bigram_query()"
      ],
      "metadata": {
        "id": "ZlboSsw2FPBK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "479ac240-ca86-44bf-a5fe-eb74a6538524"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "search your phrase queriesexperimental investigation\n",
            "Number of documents retrieved for query 1 using positional inverted index:50\n",
            "Number of documents retrieved for query 1 using positional inverted index:\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0001\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0029\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0084\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0170\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0173\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0176\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0189\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0245\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0251\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0372\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0423\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0442\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0497\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0505\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0522\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0552\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0569\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0635\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0636\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0662\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0694\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0713\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0766\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0772\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0801\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0816\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0836\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0844\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0858\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0887\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0932\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0986\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0996\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1019\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1039\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1074\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1092\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1097\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1098\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1156\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1159\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1161\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1205\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1220\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1225\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1227\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1230\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1231\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1338\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1364\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q3.(ii) Positional Indexing**"
      ],
      "metadata": {
        "id": "_N6QFzpJUDCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(newfiles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvkvsrrRwbq2",
        "outputId": "2be57bd5-c176-422e-f194-44034dcaf730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1400"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def create_positional_index(newfiles):\n",
        "    positional_index = {}\n",
        "    for i in range(len(newfiles)):\n",
        "        tokens = newfiles[i]\n",
        "        for j, word in enumerate(tokens):\n",
        "            if(word in positional_index):\n",
        "                if(i in positional_index[word]):\n",
        "                    positional_index[word][i].append(j)\n",
        "                else:\n",
        "                    positional_index[word][i] = [j]\n",
        "            else:\n",
        "                positional_index[word] = {i:[j]}\n",
        "    \n",
        "    pi_file = open('positional_index_pickle_file', 'wb')\n",
        "    pickle.dump(positional_index, pi_file)\n",
        "    pi_file.close()\n",
        "\n",
        "    return positional_index"
      ],
      "metadata": {
        "id": "T_jmYpxUVg-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positional_index = create_positional_index(newfiles)\n",
        "pi_file = open('positional_index_pickle_file', 'rb')\n",
        "positional_index = pickle.load(pi_file)\n",
        "pi_file.close()"
      ],
      "metadata": {
        "id": "FttdZV62ZgH5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection(lst1, lst2):\n",
        " \n",
        "    # Use of hybrid method\n",
        "    temp = set(lst2)\n",
        "    lst3 = [value for value in lst1 if value in temp]\n",
        "    return lst3"
      ],
      "metadata": {
        "id": "zbxNOAg119eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def difference_within_k(pos1,pos2,k):\n",
        "  for i in pos1:\n",
        "    for j in pos2:\n",
        "      if( i<j and abs(j - i) == k):\n",
        "        return 1;\n",
        "  return 0;"
      ],
      "metadata": {
        "id": "d9bQSTV1FmVQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_bigram(query,q):\n",
        "          res = len(query.split())\n",
        "          if(res>5):\n",
        "            print(\" bigram length of the input query to be <= 5\")\n",
        "            return\n",
        "            \n",
        "            \n",
        "          sanitized_query = preprocess(query, stopwords_set, 'query')\n",
        "\n",
        "          flag = 0\n",
        "          if(len(sanitized_query) == 0):\n",
        "            print(\"bigram invalid query\")\n",
        "            return\n",
        "\n",
        "          if(len(sanitized_query) == 1):\n",
        "            print(\" bigram invalid query\")\n",
        "            return\n",
        "\n",
        "          flag = 0\n",
        "          for token in sanitized_query:\n",
        "            if(token not in unigram_inverted_index):\n",
        "              print(\" bigram invalid query\")\n",
        "              return \n",
        "        \n",
        "\n",
        "          result = []\n",
        "          for i in range(len(sanitized_query)-1):\n",
        "            biword = str(sanitized_query[i] + \" \" + sanitized_query[i+1])\n",
        "            if i==0:\n",
        "              result = bigram_inverted_index[biword][1]\n",
        "            else:\n",
        "              result = result.intersection(bigram_inverted_index[biword][1])\n",
        "          \n",
        "          if(len(result)  == 0):\n",
        "            print(\" bigram no document found\")\n",
        "            return\n",
        "            \n",
        "\n",
        "          print('Number of documents retrieved for query {} using bigram inverted index: {}'.format(q,len(result)))\n",
        "          print('Number of documents retrieved for query {} using bigram inverted index:'.format(q))\n",
        "          for res_doc in result:\n",
        "            print(list_of_files[res_doc])"
      ],
      "metadata": {
        "id": "t7XVgZEsyb6u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_positional(query,q):\n",
        "          res = len(query.split())\n",
        "          if(res>5):\n",
        "              print(\"positional length of the input query to be <= 5\")\n",
        "              return\n",
        "          \n",
        "          afterPreprocessQuery = preprocess(query, stopwords_set, 'query')\n",
        "          #print(afterPreprocessQuery)\n",
        "         \n",
        "          if(len(afterPreprocessQuery) == 0):\n",
        "            print(\" positional invalid query\")\n",
        "            return\n",
        "        \n",
        "\n",
        "          pair_wise_list = []\n",
        "          pair = ()\n",
        "          words = afterPreprocessQuery\n",
        "          \n",
        "          for token in words:\n",
        "            if(token not in unigram_inverted_index):\n",
        "              print(\" positional invalid query\")\n",
        "              return\n",
        "          \n",
        "          if(len(words) == 1):\n",
        "              documents = unigram_inverted_index[words[0]][1]\n",
        "              print('Number of documents retrieved for query {} using positional inverted index:{}'.format(q,len(documents)))\n",
        "              print('Number of documents retrieved for query {} using positional inverted index:'.format(q))\n",
        "              for res_doc in documents:\n",
        "                print(list_of_files[res_doc])\n",
        "              return\n",
        "              \n",
        "        \n",
        "          first_token =  words[0]\n",
        "          result = unigram_inverted_index[first_token][1]\n",
        "          for i in range(1,len(words)):\n",
        "            token = unigram_inverted_index[words[i]][1]\n",
        "            result = intersection(result,token)\n",
        "          #print(result)\n",
        "          if(len(result) == 0):\n",
        "              print(\" positional No document found\")\n",
        "              return\n",
        "          \n",
        "\n",
        "          \n",
        "        \n",
        "          for i, word in enumerate(words[:-1]):\n",
        "            pair = (word, words[i+1])\n",
        "            pair_wise_list.append(pair)\n",
        "          temp = (words[0],words[-1])\n",
        "          pair_wise_list.append(temp)\n",
        "\n",
        "        \n",
        "          result_documents = []\n",
        "          count = 0\n",
        "          for doc_id in result:\n",
        "            for pair in pair_wise_list:\n",
        "              if(pair == pair_wise_list[-1]):\n",
        "                continue\n",
        "              position_tk1 = positional_index[pair[0]][doc_id]\n",
        "              position_tk2 = positional_index[pair[1]][doc_id]\n",
        "\n",
        "              if(difference_within_k(position_tk1,position_tk2,1)):\n",
        "                count = count + 1\n",
        "            \n",
        "            k = len(words) - 1\n",
        "            pair = pair_wise_list[-1]\n",
        "            #print(doc_id)\n",
        "            if(difference_within_k(positional_index[pair[0]][doc_id],positional_index[pair[1]][doc_id],k)):\n",
        "              count = count + 1\n",
        "            # print(doc_id,\"-->\",count)\n",
        "            if(count == len(pair_wise_list)-1 and len(words) == 2):\n",
        "              result_documents.append(doc_id)\n",
        "            if(count == len(pair_wise_list) and len(words)> 2):\n",
        "              result_documents.append(doc_id)\n",
        "            count = 0\n",
        "\n",
        "          print('Number of documents retrieved for query {} using positional inverted index:{}'.format(q,len(result_documents)))\n",
        "          print('Number of documents retrieved for query {} using positional inverted index:'.format(q))\n",
        "          for res_doc in result_documents:\n",
        "              print(list_of_files[res_doc])\n"
      ],
      "metadata": {
        "id": "0yH95-JPywRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_query():\n",
        "    # create set of stop words for preprocessing\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    \n",
        "    # Get List of Files in Dataset\n",
        "    #list_of_files = getListOfFiles('/content/drive/MyDrive/Information Retrieval/Assignment-1/Dataset_with_change/CSE508_Winter2023_Dataset/')\n",
        "    \n",
        "    # create dictizzonary of file with docID (integer) as key and full_path of file as value\n",
        "    # file_dictionary = create_file_dictionary(list_of_files)\n",
        "    \n",
        "    # create positional index once and then load pickle file afterwards\n",
        "    #Loading pre-processed files\n",
        "    # positional_index = create_positional_index(newfiles)\n",
        "    # pi_file = open('positional_index_pickle_file', 'rb')\n",
        "    # positional_index = pickle.load(pi_file)\n",
        "    # pi_file.close()\n",
        "      \n",
        "    N = int(input(\"input number of queries\")) #input your query\n",
        "    inputs = []\n",
        "    for q in range(N):\n",
        "        input_query = input(\"search your phrase queries\")\n",
        "        inputs.append(input_query)\n",
        "\n",
        "    count = 1\n",
        "    for query in inputs:\n",
        "        search_bigram(query,count)\n",
        "        print(\"----------xxxxxxx--------\")\n",
        "        search_positional(query,count)\n",
        "        count = count +  1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M7HEIp2O1Y3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = search_query()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ibo0CBJo70ZQ",
        "outputId": "86fe1b80-a7a5-492b-f7c9-3db1197c2805"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input number of queries3\n",
            "search your phrase querieswqdd  qd2j dqj di1hvqjh  vdhj wcj qhwvfjh fk\n",
            "search your phrase queriestransition supersonic\n",
            "search your phrase queriesexperimental transition\n",
            " bigram length of the input query to be <= 5\n",
            "----------xxxxxxx--------\n",
            "positional length of the input query to be <= 5\n",
            "Number of documents retrieved for query 2 using bigram inverted index: 5\n",
            "Number of documents retrieved for query 2 using bigram inverted index:\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0007\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0040\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0041\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0182\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1211\n",
            "----------xxxxxxx--------\n",
            "Number of documents retrieved for query 2 using positional inverted index:5\n",
            "Number of documents retrieved for query 2 using positional inverted index:\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0007\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0040\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0041\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0182\n",
            "/content/CSE508_Winter2023_Dataset/cranfield1211\n",
            "Number of documents retrieved for query 3 using bigram inverted index: 1\n",
            "Number of documents retrieved for query 3 using bigram inverted index:\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0504\n",
            "----------xxxxxxx--------\n",
            "Number of documents retrieved for query 3 using positional inverted index:1\n",
            "Number of documents retrieved for query 3 using positional inverted index:\n",
            "/content/CSE508_Winter2023_Dataset/cranfield0504\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sstrapositional_index['transition']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ewr3COAtu7dv",
        "outputId": "3dd5694d-d115-4180-f012-6a1d551d1cbb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{6: [6, 28, 48, 69, 124],\n",
              " 7: [8, 12, 35, 59],\n",
              " 8: [0, 12, 36, 102, 107],\n",
              " 23: [86],\n",
              " 39: [3, 24, 53, 67, 75],\n",
              " 40: [0, 6, 33],\n",
              " 42: [7, 13, 42, 77],\n",
              " 52: [0, 18, 61, 122],\n",
              " 78: [6, 21, 37, 53, 65, 84, 95],\n",
              " 79: [8, 32, 53, 113, 143],\n",
              " 88: [76],\n",
              " 93: [228],\n",
              " 95: [5, 19, 31, 71, 93, 110, 127, 135],\n",
              " 122: [42, 121],\n",
              " 124: [84, 94, 108],\n",
              " 132: [87],\n",
              " 141: [62],\n",
              " 170: [75],\n",
              " 181: [2, 24, 39],\n",
              " 186: [8, 62, 75, 113],\n",
              " 198: [37, 62, 71, 139, 187],\n",
              " 206: [4, 21, 48, 56],\n",
              " 211: [104, 112],\n",
              " 243: [112, 187],\n",
              " 260: [48],\n",
              " 271: [16,\n",
              "  23,\n",
              "  33,\n",
              "  52,\n",
              "  65,\n",
              "  73,\n",
              "  79,\n",
              "  84,\n",
              "  108,\n",
              "  144,\n",
              "  152,\n",
              "  162,\n",
              "  175,\n",
              "  189,\n",
              "  198,\n",
              "  228,\n",
              "  231,\n",
              "  237,\n",
              "  247],\n",
              " 292: [6, 15, 31, 44],\n",
              " 293: [140],\n",
              " 313: [10, 27, 98],\n",
              " 314: [8, 51, 70, 102, 125, 136, 163, 212, 236, 244, 259],\n",
              " 329: [53],\n",
              " 336: [2, 30],\n",
              " 337: [44, 67],\n",
              " 340: [48, 108],\n",
              " 343: [91],\n",
              " 345: [21, 62],\n",
              " 417: [0, 9, 33],\n",
              " 439: [39],\n",
              " 440: [106],\n",
              " 503: [89, 96],\n",
              " 504: [0, 13, 36, 38],\n",
              " 521: [178, 184, 191],\n",
              " 524: [39],\n",
              " 525: [43],\n",
              " 534: [35],\n",
              " 535: [0, 7, 94, 113, 138],\n",
              " 557: [3, 13],\n",
              " 609: [69],\n",
              " 667: [39],\n",
              " 689: [50, 64],\n",
              " 709: [6, 15, 96],\n",
              " 743: [64],\n",
              " 765: [72, 145],\n",
              " 773: [10],\n",
              " 793: [99],\n",
              " 795: [12, 29],\n",
              " 797: [213],\n",
              " 826: [171],\n",
              " 857: [70],\n",
              " 858: [95, 130],\n",
              " 932: [145],\n",
              " 958: [93, 108],\n",
              " 991: [27, 76, 84],\n",
              " 995: [57, 68],\n",
              " 1039: [181],\n",
              " 1088: [33, 61],\n",
              " 1092: [7, 14],\n",
              " 1161: [52, 57],\n",
              " 1162: [53, 64],\n",
              " 1169: [68],\n",
              " 1187: [67],\n",
              " 1200: [134, 159, 189, 221, 230, 256],\n",
              " 1204: [4, 15, 36, 44, 57, 69, 90],\n",
              " 1210: [2, 27, 42, 71],\n",
              " 1213: [100],\n",
              " 1219: [2, 11],\n",
              " 1256: [55],\n",
              " 1263: [2, 28, 33, 52, 68, 81, 93, 102],\n",
              " 1267: [148],\n",
              " 1277: [0, 5, 22, 28, 35, 51],\n",
              " 1283: [0, 31, 39, 67],\n",
              " 1286: [31],\n",
              " 1299: [4, 12, 59, 74, 116, 169],\n",
              " 1320: [14],\n",
              " 1323: [1],\n",
              " 1324: [8],\n",
              " 1380: [5, 23, 63, 79, 84, 109, 151]}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LATVfI0evhvO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}