{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohammed-Taasir/CSE508_Winter2023_A1_14/blob/assignment-1/IR_Assignment_1_Question_3_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEuzfIdQ9Edl",
        "outputId": "ffd61d73-e268-454d-9e99-cb2a0ddcc6d9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/drive/MyDrive/IR Assignment-1/CSE508_Winter2023_Dataset.zip' '/content/'\n",
        "!unzip 'CSE508_Winter2023_Dataset.zip' &> /dev/null\n",
        "!rm 'CSE508_Winter2023_Dataset.zip'"
      ],
      "metadata": {
        "id": "EtcInnCQ9Eb9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/CSE508_Winter2023_Dataset/'"
      ],
      "metadata": {
        "id": "1HeUqmvp9EQv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "from sortedcontainers import SortedDict, SortedList, SortedSet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import PlaintextCorpusReader \n",
        "from nltk.stem.wordnet import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "6jI22ep79EPG"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZxIOOsi9ED7",
        "outputId": "0d6267e3-ca5e-49dd-daaa-b4d3e8d25701"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getListOfFiles(directory):\n",
        "\n",
        "    # Parameters: directory: type(string)        \n",
        "    # returns: list of all files in directory with the full path of file\n",
        "    \n",
        "    list_of_files = []\n",
        "\n",
        "    for i in range(len(filenames)):\n",
        "      fi = \"/content/CSE508_Winter2023_Dataset/\"+filenames[i]\n",
        "      list_of_files.append(fi)\n",
        "    \n",
        "    return list_of_files"
      ],
      "metadata": {
        "id": "Vl_baKlC9ECW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowercase(data):\n",
        "\n",
        "    # Parameters: data: type(string)\n",
        "    # returns: lowercase of data   \n",
        "     \n",
        "    return data.lower()\n",
        "\n",
        "def perform_word_tokenize(corpus):\n",
        "  \n",
        "    # Parameters:corpus: type(string)   \n",
        "    # returns word-level tokenization of corpus\n",
        "\n",
        "    return word_tokenize(corpus)\n",
        "\n",
        "def remove_stopwords_from_tokens(tokens, stopwords_set):\n",
        "  \n",
        "    # Parameters: tokens: type(list)\n",
        "    #             stopwords_set: type(set)\n",
        "    # returns: tokens without stopwords\n",
        "\n",
        "    tokens_sans_stopwords = [x for x in tokens if x not in stopwords_set] \n",
        "    return tokens_sans_stopwords\n",
        "\n",
        "def remove_punctuation_from_tokens(tokens):\n",
        "\n",
        "    # Parameters: tokens: type(list)\n",
        "    # returns: tokens without punctuation\n",
        "\n",
        "    tokens_sans_punctuation = [x.translate(str.maketrans('', '', string.punctuation)) for x in tokens]\n",
        "    return tokens_sans_punctuation\n",
        "\n",
        "def remove_blank_space_tokens(tokens):\n",
        "    \n",
        "    #Parameters: tokens: type(list)\n",
        "    #returns: tokens without blank tokens\n",
        "\n",
        "    tokens_sans_blank_space = [x for x in tokens if x!='']  \n",
        "    return tokens_sans_blank_space"
      ],
      "metadata": {
        "id": "VK44G1qQ-A2g"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(corpus, stopwords_set, preprocess_type):\n",
        "    # Convert the text to lower case\n",
        "    lowercase_corpus = lowercase(corpus)\n",
        "    \n",
        "    # Perform word tokenization (word_tokenize also takes care of whitespace)\n",
        "    word_tokens = perform_word_tokenize(lowercase_corpus)\n",
        "    \n",
        "    # Remove stopwords from tokens\n",
        "    word_tokens_sans_stopwords = remove_stopwords_from_tokens(word_tokens, stopwords_set)\n",
        "    \n",
        "    # Remove punctuation marks from tokens\n",
        "    word_tokens_sans_punctuation = remove_punctuation_from_tokens(word_tokens_sans_stopwords)\n",
        "    \n",
        "    # Remove blank space tokens\n",
        "    word_tokens_sans_blank_tokens = remove_blank_space_tokens(word_tokens_sans_punctuation)\n",
        "  \n",
        "    \n",
        "    if preprocess_type=='query':\n",
        "        return word_tokens_sans_blank_tokens                              # if its a query return tokens in the same order\n",
        "    return sorted(list(dict.fromkeys(word_tokens_sans_blank_tokens)))     # if its a whole document then return tokens in sorted manner"
      ],
      "metadata": {
        "id": "Tpw6Y63t-A01"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_file_dictionary(list_of_files):\n",
        "    \n",
        "    # Paramteres: list_of_files: type(string)\n",
        "    # returns: file_dictionary with integer key and path_of_file as value\n",
        "  \n",
        "    file_dictionary = {}\n",
        "    for i in range(len(list_of_files)):\n",
        "        file_dictionary[i] = list_of_files[i]\n",
        "    \n",
        "    return file_dictionary"
      ],
      "metadata": {
        "id": "jU3PlZD6-Axa"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "corpus_root='/content/CSE508_Winter2023_Dataset/'\n",
        "corpus=PlaintextCorpusReader(corpus_root,'.*')"
      ],
      "metadata": {
        "id": "gGBHemjE-AvX"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing names of all files\n",
        "\n",
        "filenames=corpus.fileids()\n",
        "# print(filenames)"
      ],
      "metadata": {
        "id": "QGjkMQx_-As9"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(filenames))\n",
        "print(len(filenames))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNuoRyoa-Aq4",
        "outputId": "4d61d331-1c60-4c1e-fcb9-d46e99e8de58"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(filenames)):\n",
        "    outfile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "    doc = outfile.read()\n",
        "\n",
        "    # initializing string\n",
        "    test_str = doc\n",
        "\n",
        "    # initializing tags\n",
        "    tag1 = \"TITLE\"\n",
        "    tag2 = \"TEXT\"\n",
        "\n",
        "    # regex to extract required strings\n",
        "    reg_str1 = \"<\"+tag1+\">(.*?)</\"+tag1+\">\"\n",
        "    res1 = re.findall(reg_str1, test_str, re.DOTALL)\n",
        "\n",
        "    reg_str2 = \"<\"+tag2+\">(.*?)</\"+tag2+\">\"\n",
        "    res2 = re.findall(reg_str2, test_str, re.DOTALL)\n",
        "\n",
        "    #Combining contents of TITLE and TEXT\n",
        "    res = res1+res2\n",
        "    \n",
        "    s = res\n",
        "\n",
        "    # using list comprehension\n",
        "    listToStr = ' '.join([str(elem) for elem in s])\n",
        "\n",
        "    writeFile = open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'w')\n",
        "    L = listToStr\n",
        "\n",
        "    writeFile.write(L)\n",
        "    writeFile.close()\n",
        "\n",
        "    outfile.close()"
      ],
      "metadata": {
        "id": "pf-MZYgB-Am6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords_set = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "wWmv52MUhREU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "newfiles=[]"
      ],
      "metadata": {
        "id": "eQr8lnp3-wy6"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#adding all docs in a list\n",
        "for i in range(len(filenames)):\n",
        "  fileind=open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  filedata=fileind.read()\n",
        "  filedata=lowercase(filedata)\n",
        "  newfiles.append(filedata)\n",
        "print(len(newfiles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D04c1o0z-wvo",
        "outputId": "a75d3589-1fbd-4665-e8a5-14d0ccd070a9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#adding all docs after preprocessing in a list\n",
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  filedata=perform_word_tokenize(filedata)\n",
        "  tokens_sans_stopwords = remove_stopwords_from_tokens(filedata, stopwords_set)\n",
        "  tokens_sans_punctuation = remove_punctuation_from_tokens(tokens_sans_stopwords)\n",
        "  tokens_sans_blank_space = remove_blank_space_tokens(tokens_sans_punctuation)\n",
        "  newfiles[i] = tokens_sans_blank_space\n",
        "print(len(newfiles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kFiLi5t-wrS",
        "outputId": "5f1478f1-dcac-4d64-c813-4f059ff2a4b4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fileData = newfiles"
      ],
      "metadata": {
        "id": "d-sNeFpJ_Gb4"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for i in range(len(newfiles)):\n",
        "#   print('File : '+str(i+1)+' ', end='')\n",
        "#   print(newfiles[i])"
      ],
      "metadata": {
        "id": "IKofU3La_GZ7"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_unigram_inverted_index(file_dictionary, stopwords_set):\n",
        "    # initialize unigram inverted index\n",
        "    unigram_inverted_index = {}\n",
        "    \n",
        "    # unigram inverted index\n",
        "    for doc_ID in range(len(file_dictionary)):\n",
        "        file = open(file_dictionary[doc_ID], 'r', encoding='utf-8', errors='ignore')\n",
        "        file_corpus = file.read()\n",
        "        file.close()\n",
        "        doc_tokens = preprocess(file_corpus, stopwords_set, 'doc')\n",
        "        for token in doc_tokens:\n",
        "            if token in unigram_inverted_index:\n",
        "                unigram_inverted_index[token][0] += 1               # stores doc_frequency\n",
        "                unigram_inverted_index[token][1].append(doc_ID)     # appending docs in which that term occurs\n",
        "            else:\n",
        "                unigram_inverted_index[token] = []\n",
        "                unigram_inverted_index[token].append(1)\n",
        "                unigram_inverted_index[token].append([doc_ID])      # initializing if this token appears for first time\n",
        "    \n",
        "    # Saving unigram inverted index using pickle module  \n",
        "    uii_file = open('unigram_inverted_index_pickle_file', 'wb')\n",
        "    pickle.dump(unigram_inverted_index, uii_file)\n",
        "    uii_file.close()\n",
        "\n",
        "    return unigram_inverted_index\n",
        "\n",
        "stopwords_set = set(stopwords.words('english')) #getting all stopwords\n",
        "\n",
        "list_of_files = getListOfFiles('/content/CSE508_Winter2023_Dataset/')\n",
        "uni = create_unigram_inverted_index(list_of_files,stopwords_set)"
      ],
      "metadata": {
        "id": "lefneA5f-wi0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uii_file = open('unigram_inverted_index_pickle_file', 'rb') #loading pickle model\n",
        "unigram_inverted_index = pickle.load(uii_file)\n",
        "uii_file.close()"
      ],
      "metadata": {
        "id": "Tu5bfmMV-AlM"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(unigram_inverted_index))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2WmZdI-9D-4",
        "outputId": "ac17ddcd-a073-4f48-832a-1b91761a5523"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8996\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# (QUESTION 3) PHRASE QUERIES"
      ],
      "metadata": {
        "id": "LnRDHrl56kNP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (i) Bigram inverted index [30 Marks]\n",
        "1. Create a bigram inverted index (from scratch; No library allowed) of the dataset obtained\n",
        "from Q1.\n",
        "2. Use Python’s pickle module to save and load the bigram inverted index."
      ],
      "metadata": {
        "id": "9ER5fLpu6xVv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newlist=[]\n",
        "newlist=newfiles\n",
        "\n",
        "for i in range(len(newfiles)):\n",
        "   for j in range(len(newfiles[i])-1):\n",
        "       temp1=newfiles[i][j]\n",
        "       temp2=newfiles[i][j+1]\n",
        "       temp=temp1+' '+temp2\n",
        "       newlist[i][j]=temp\n",
        "   newlist[i].pop()\n",
        "\n",
        "bigram_inverted_index = {}\n",
        "for i in range(len(newlist)):\n",
        "    doc_tokens = newlist[i]\n",
        "    for token in doc_tokens:\n",
        "        if token in bigram_inverted_index:\n",
        "            bigram_inverted_index[token][0] += 1   # stores doc_frequency\n",
        "            if( i not in bigram_inverted_index[token][1]):\n",
        "              bigram_inverted_index[token][1].append(i)    # appending docs in which that term occurs\n",
        "              \n",
        "        else:\n",
        "            bigram_inverted_index[token] = []\n",
        "            bigram_inverted_index[token].append(1)\n",
        "            bigram_inverted_index[token].append([i])    # initializing if this token appears for first time'\n",
        "\n",
        "pi_file = open('bigram_inverted_index_pickle_file', 'wb') #saving the model\n",
        "pickle.dump(bigram_inverted_index, pi_file)\n",
        "pi_file.close()"
      ],
      "metadata": {
        "id": "dhfiCeIhHHDI"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bigram_file = open('bigram_inverted_index_pickle_file', 'rb')\n",
        "bigram_inverted_index = pickle.load(bigram_file)\n",
        "bigram_file.close()"
      ],
      "metadata": {
        "id": "L3txIWWQ6duk"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(bigram_inverted_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0RazGMdMWqyL",
        "outputId": "bca4a0a5-889f-4372-c375-0b7fc3cd9a6d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "85209"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uii_file = open('unigram_inverted_index_pickle_file', 'rb')\n",
        "unigram_inverted_index = pickle.load(uii_file)\n",
        "uii_file.close()"
      ],
      "metadata": {
        "id": "ALdQx6HI6dv1"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  # create set of stop words for preprocessing\n",
        "stopwords_set = set(stopwords.words('english'))\n",
        "\n",
        "  # Get List of Files in Dataset\n",
        "list_of_files = getListOfFiles('/content/CSE508_Winter2023_Dataset/')\n",
        "\n",
        "  # create dictionary of file with docID (integer) as key and full_path of file as value\n",
        "file_dictionary = create_file_dictionary(list_of_files)"
      ],
      "metadata": {
        "id": "2XypOr5wXRw5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_bigram(query,q):\n",
        "          res = len(query.split())\n",
        "          if(res>5):\n",
        "            print(\"bigram invalid query \"+str(q)+\" -> Length of input sequence has to be <=5\\n\")\n",
        "            return\n",
        "                     \n",
        "          sanitized_query = preprocess(query, stopwords_set, 'query')\n",
        "\n",
        "          flag = 0\n",
        "          if(len(sanitized_query) == 0):\n",
        "            print(\"bigram invalid query \"+str(q)+\" -> you had entered stopwords in your query\\n\")\n",
        "            return\n",
        "\n",
        "          if(len(sanitized_query) == 1):\n",
        "            print(\"bigram invalid query \"+str(q)+\" -> length of your query is 1, for bigram creation length of input query has to be atleast 2\\n\")\n",
        "            return\n",
        "\n",
        "          flag = 0\n",
        "          for token in sanitized_query:\n",
        "            if(token not in unigram_inverted_index):\n",
        "              print(\"bigram invalid query \"+str(q)+\" -> given token \"+token+\" is not present in our unigram_inverted_index hence no corresponding bigram_inverted_index entry will be found.\\n\")\n",
        "              return \n",
        "\n",
        "          result = []\n",
        "          for i in range(len(sanitized_query)-1):\n",
        "            biword = str(sanitized_query[i] + \" \" + sanitized_query[i+1])\n",
        "            if(biword not in bigram_inverted_index):\n",
        "              print(\"bigram invalid query \"+str(q)+\" -> biword \"+biword+\" is not present in our bigram_inverted_index.\\n\")\n",
        "              break\n",
        "            if i==0:\n",
        "              result = bigram_inverted_index[biword][1]\n",
        "            else:\n",
        "              result = intersection(result, bigram_inverted_index[biword][1])\n",
        "          \n",
        "          if(len(result)  == 0):\n",
        "            print(\"No document found in bigram_inverted_index.\\n\")\n",
        "            return\n",
        "            \n",
        "          print('Number of documents retrieved for query {} using bigram inverted index : {}'.format(q,len(result)))\n",
        "          print('Name of documents retrieved for query {} using bigram inverted index : '.format(q), end='')\n",
        "\n",
        "          output = []\n",
        "          for res_doc in result:\n",
        "            output.append(list_of_files[res_doc])\n",
        "          print(output)"
      ],
      "metadata": {
        "id": "QIAsccGg7toN"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (bigram_inverted_index)"
      ],
      "metadata": {
        "id": "o0Bk9njQAGas"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (ii) Positional index [35 Marks]\n",
        "1. Create a positional index (from scratch; No library allowed) of the dataset obtained from\n",
        "Q1.\n",
        "2. Use Python’s pickle module to save and load the positional index."
      ],
      "metadata": {
        "id": "139ZAZVL7V0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newfiles=[]"
      ],
      "metadata": {
        "id": "X2JPwM87kpR9"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(filenames)):\n",
        "  fileind=open('/content/CSE508_Winter2023_Dataset/'+filenames[i], 'r')\n",
        "  filedata=fileind.read()\n",
        "  filedata=lowercase(filedata)\n",
        "  newfiles.append(filedata)\n",
        "print(len(newfiles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b1fe2b-1c4e-407d-ff93-822a2ae25b21",
        "id": "rRMZo0NlkpR9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(newfiles)):\n",
        "  filedata=newfiles[i]\n",
        "  filedata=perform_word_tokenize(filedata)\n",
        "  tokens_sans_stopwords = remove_stopwords_from_tokens(filedata, stopwords_set)\n",
        "  tokens_sans_punctuation = remove_punctuation_from_tokens(tokens_sans_stopwords)\n",
        "  tokens_sans_blank_space = remove_blank_space_tokens(tokens_sans_punctuation)\n",
        "  newfiles[i] = tokens_sans_blank_space\n",
        "print(len(newfiles))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70d0e35f-dc0a-458f-b8bb-3e9f2229fc1e",
        "id": "ytK0su76kpR9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(newfiles)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSKSX-gF6dzi",
        "outputId": "6ee79adf-ed49-404e-ef50-196c1fa099c9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1400"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "def create_positional_index(newfiles):\n",
        "    positional_index = {}\n",
        "    for i in range(len(newfiles)):\n",
        "        tokens = newfiles[i]\n",
        "        for j, word in enumerate(tokens):\n",
        "            if(word in positional_index):\n",
        "                if(i in positional_index[word]):\n",
        "                    positional_index[word][i].append(j)\n",
        "                else:\n",
        "                    positional_index[word][i] = [j]\n",
        "            else:\n",
        "                positional_index[word] = {i:[j]}\n",
        "    \n",
        "    pi_file = open('positional_index_pickle_file', 'wb')\n",
        "    pickle.dump(positional_index, pi_file)\n",
        "    pi_file.close()\n",
        "\n",
        "    return positional_index"
      ],
      "metadata": {
        "id": "WZ1vx1aG6d1C"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positional_index = create_positional_index(newfiles)\n",
        "pi_file = open('positional_index_pickle_file', 'rb')\n",
        "positional_index = pickle.load(pi_file)\n",
        "pi_file.close()"
      ],
      "metadata": {
        "id": "EZdzzo1O6d4l"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(positional_index)"
      ],
      "metadata": {
        "id": "LZzMPljQB79q"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def intersection(lst1, lst2): \n",
        "    # Use of hybrid method\n",
        "    temp = set(lst2)\n",
        "    lst3 = [value for value in lst1 if value in temp]\n",
        "    return lst3"
      ],
      "metadata": {
        "id": "Uf_Wp8vi6d6D"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def difference_within_k(pos1,pos2,k):\n",
        "  for i in pos1:\n",
        "    for j in pos2:\n",
        "      if( i<j and abs(j - i) == k):\n",
        "        return 1;\n",
        "  return 0;"
      ],
      "metadata": {
        "id": "16s4Z84f6d9f"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search_positional(query,q):\n",
        "          res = len(query.split())\n",
        "          if(res>5):\n",
        "              print(\"positional invalid query \"+str(q)+\" -> Length of input sequence has to be <=5\\n\")\n",
        "              return\n",
        "          \n",
        "          afterPreprocessQuery = preprocess(query, stopwords_set, 'query')\n",
        "         \n",
        "          if(len(afterPreprocessQuery) == 0):\n",
        "            print(\"positional invalid query \"+str(q)+\" -> you had entered stopwords in your query\\n\")\n",
        "            return\n",
        "        \n",
        "          pair_wise_list = []\n",
        "          pair = ()\n",
        "          words = afterPreprocessQuery\n",
        "          \n",
        "          for token in words:\n",
        "            if(token not in unigram_inverted_index):\n",
        "              print(\"positional invalid query \"+str(q)+\" -> given token \"+token+\" is not present in our positional index\\n\")\n",
        "              return\n",
        "          \n",
        "          if(len(words) == 1):\n",
        "              documents = unigram_inverted_index[words[0]][1]\n",
        "              print('Number of documents retrieved for query {} using positional inverted index : {}'.format(q,len(documents)))\n",
        "              print('Name of documents retrieved for query {} using positional inverted index : '.format(q), end='')\n",
        "\n",
        "              out = []\n",
        "              for res_doc in documents:\n",
        "                out.append(list_of_files[res_doc])\n",
        "              print(out)\n",
        "              return\n",
        "              \n",
        "        \n",
        "          first_token =  words[0]\n",
        "          result = unigram_inverted_index[first_token][1]\n",
        "          for i in range(1,len(words)):\n",
        "            token = unigram_inverted_index[words[i]][1]\n",
        "            result = intersection(result,token)  #finding all the documents that are common in all the tokens\n",
        "\n",
        "          if(len(result) == 0):\n",
        "              print(\"No document found in our positional index.\\n\")\n",
        "              return        \n",
        "        \n",
        "          for i, word in enumerate(words[:-1]):\n",
        "            pair = (word, words[i+1])\n",
        "            pair_wise_list.append(pair)\n",
        "          temp = (words[0],words[-1])\n",
        "          pair_wise_list.append(temp)\n",
        "\n",
        "        \n",
        "          result_documents = []\n",
        "          count = 0\n",
        "          for doc_id in result:\n",
        "            for pair in pair_wise_list:\n",
        "              if(pair == pair_wise_list[-1]):\n",
        "                continue\n",
        "              position_tk1 = positional_index[pair[0]][doc_id]\n",
        "              position_tk2 = positional_index[pair[1]][doc_id]\n",
        "\n",
        "              if(difference_within_k(position_tk1,position_tk2,1)): \n",
        "                count = count + 1\n",
        "            \n",
        "            k = len(words) - 1\n",
        "            pair = pair_wise_list[-1]\n",
        " \n",
        "            if(difference_within_k(positional_index[pair[0]][doc_id],positional_index[pair[1]][doc_id],k)): \n",
        "              count = count + 1\n",
        "            if(count == len(pair_wise_list)-1 and len(words) == 2):\n",
        "              result_documents.append(doc_id)\n",
        "            if(count == len(pair_wise_list) and len(words)> 2):\n",
        "              result_documents.append(doc_id)\n",
        "            count = 0\n",
        "\n",
        "          print('Number of documents retrieved for query {} using positional inverted index : {}'.format(q,len(result_documents)))\n",
        "          print('Name of documents retrieved for query {} using positional inverted index : '.format(q), end='')\n",
        "\n",
        "          output = []\n",
        "          for res_doc in result_documents:\n",
        "            output.append(list_of_files[res_doc])\n",
        "          print(output)\n"
      ],
      "metadata": {
        "id": "oirSkcpw6d_B"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### (iii) Compare and comment on your results using (i) and (ii). [5 Marks]\n",
        "1. Input Format:<br>\n",
        "a. The first line contains N denoting the number of queries to execute<br>\n",
        "b. The next N lines contain phrase queries<br>\n",
        "2. Output Format:<br>\n",
        "a. 4N lines consisting of the results in the following format:<br>\n",
        "&ensp;i. Number of documents retrieved for query X using bigram inverted index<br>\n",
        "&ensp;ii. Names of documents retrieved for query X using bigram inverted index<br>\n",
        "&ensp;iii. Number of documents retrieved for query X using positional index<br>\n",
        "&ensp;iv. Names of documents retrieved for query X using positional index<br>\n",
        "3. Perform preprocessing steps (from Q1) on the input sequence as well. Assume the length of the input sequence to be <=5."
      ],
      "metadata": {
        "id": "qnjZb1Kl8Azy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search_query():\n",
        "    # create set of stop words for preprocessing\n",
        "    stopwords_set = set(stopwords.words('english'))\n",
        "    \n",
        "    N = int(input(\"Enter number of phrase queries : \")) #input your query\n",
        "    inputs = []\n",
        "    for q in range(N):\n",
        "        input_query = input(\"Enter query \"+str(q+1)+\" : \")\n",
        "        inputs.append(input_query)\n",
        "\n",
        "    print('\\n')\n",
        "\n",
        "    count = 1\n",
        "    for query in inputs:\n",
        "        search_bigram(query,count)\n",
        "        # print('\\n')\n",
        "        search_positional(query,count)\n",
        "        count = count +  1\n",
        "        print('\\n')"
      ],
      "metadata": {
        "id": "mx_1mnhO6eCa"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a = search_query()"
      ],
      "metadata": {
        "id": "7zo-AbAQ6eEb"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = search_query()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaRO-obOUGI7",
        "outputId": "52f78b5e-3964-4f6c-e9d2-ec9228db9e2c"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of phrase queries : 2\n",
            "Enter query 1 : Downstream,,, flight... velocities considered.,., vary\n",
            "Enter query 2 : spread],] sources frequency))\n",
            "\n",
            "\n",
            "Number of documents retrieved for query 1 using bigram inverted index : 1\n",
            "Name of documents retrieved for query 1 using bigram inverted index : ['/content/CSE508_Winter2023_Dataset/cranfield0085']\n",
            "Number of documents retrieved for query 1 using positional inverted index : 1\n",
            "Name of documents retrieved for query 1 using positional inverted index : ['/content/CSE508_Winter2023_Dataset/cranfield0085']\n",
            "\n",
            "\n",
            "Number of documents retrieved for query 2 using bigram inverted index : 1\n",
            "Name of documents retrieved for query 2 using bigram inverted index : ['/content/CSE508_Winter2023_Dataset/cranfield0129']\n",
            "Number of documents retrieved for query 2 using positional inverted index : 0\n",
            "Name of documents retrieved for query 2 using positional inverted index : []\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = search_query()"
      ],
      "metadata": {
        "id": "4YzMYpNjUdam",
        "outputId": "7fb1926f-8120-4327-ab65-e87bc6dc5b27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter number of phrase queries : 1\n",
            "Enter query 1 : beautiful rare documents\n",
            "\n",
            "\n",
            "bigram invalid query 1 -> given token beautiful is not present in our unigram_inverted_index hence no corresponding bigram_inverted_index entry will be found.\n",
            "\n",
            "positional invalid query 1 -> given token beautiful is not present in our positional index\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}